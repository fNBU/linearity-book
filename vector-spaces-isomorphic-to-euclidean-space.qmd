<!-- the hidden latex block causes errors when rendering to pdf, so we force it only to render to html -->
::: {.content-visible when-format="html"} 

<!-- the following block is for page defs which do not render anything upon definition  -->
::: {.hidden} 
$$
\DeclareMathOperator{\Span}{Span}
$$
:::

:::

# Vector spaces isomorphic to Euclidean space

No, there are still more mysteries!

One example is the difference between $P_1$ and $\mathbb R^2$.
We have the following ordered bases for these

$$
[x,1] \text{ for } P_1 \text{ and } [(1,0),(0,1)] \text{ for } \mathbb R^2.
$$

When we have fixed a basis for a vector space, @def-basis together with @thm-linear-indep-defs and @def-span tell us that, for each vector in our vector space, there is a exactly one way to write that vector as a linear combination of the basis vectors.
So, for example, the vector $x - 3 \in P_1$ can be thought of as $(1)\underbrace{(x)}_{\text{1st basis vector}} + (-3)\underbrace{(1)}_{\text{2nd basis vector}}$.

If you and I are thinking of the same basis $[x,1]$ for $P_1$, then there is no confusion if we just list the coefficients of this vector relative to this basis.
That is, we use the notation $(1,-3)$ to *refer* to the vector $x - 3 \in P_1$.

But $(1,-3)$ is a list of two real numbers, so isn't this an element in $\mathbb R^2$?
Is $P_1$ the same as $\mathbb R^2$?

## Dimension

One remarkable fact about $P_1$ is that, although $[x,1]$ isn't the only basis for it (the list $[x+1, x-1]$, we have seen, is another), **all** the bases of $P_1$ have something in common: they have exactly two elements.
This can be generalized to the following lemma:

::: {#lem-dimension}

Let $V$ be a vector space with at least one finite basis and let $B$ be that basis.
Then all bases of $V$ have the same number of elements as $B$.[^dimension-cardinality]

:::

[^dimension-cardinality]: For readers familiar with the notion of cardinality, this lemma can be generalized to infinite bases where it states that any two bases have the same cardinality.

This immediately tells us that all bases for $P_1$ have two elements, as well as all bases for $\mathbb R^2$.

Since all bases for a vector space are the same length we can refer to this length as a property of the vector space, rather than a property of a particular basis.

::: {#def-dimension}

Let $V$ be a vector space with a finite basis of size $n$.
Then we say that the *dimension* of $V$ is $n$ and we write this like
$$
\dim V = n.
$$

:::

So, using this notation, $\dim P_1 = 2 = \dim \mathbb R^2$.

::: {#exr-dim-of-pn}

What is the dimension of $P_2$?
What about $P_3$?
What about $P_n$?

:::

::: {#exr-dim-of-arrows}

What is the dimension of the vector space of arrows drawn on a piece of paper?
What is the dimension of the vector space of arrows created in a space like a room? (You can think of bits of string of different lengths, with arrows on one end of them, held in space by two people. Addition and multiplication is as for arrows on paper.)

:::

::: {#exr-dim-of-cc}

What is the dimension of $\mathbb C$ as a vector space over itself?

:::

::: {#exr-dim-of-cr}

What is the dimension of $\mathbb C$ as a vector space over $\mathbb R$?

:::

The following lemma says that if one vector space is inside another, then the dimension of the one on the inside can't be larger than the one on the outside:

::: {#lem-subspace-dim-ineq}

Let $W \subset V$ be a vector subspace of the vector space $V$.
Suppose $W$ had finite dimension.
Then
$$
\dim W \leq \dim V.
$$

:::

::: {#exr-dim-of-continuous}

The dimension of the space of continuous functions $\mathbb R \to \mathbb R$ is not finite.
Use the result of @exr-dim-of-pn to explain why.

:::

::: {#exr-dim-of-kernel-of-d}

> Recall that we know that the function $D : C^1 \to C^0$ "take the derivative" is linear.
> Let $K$ be the set of all vectors in $C^1$ which are taken to the 0 vector in $C^0$ by $D$.
> That is, $K$ is the set of all differentiable functions whose first derivative is 0 everywhere.
> As an equation,
> $$
> K := \left\{ f : \mathbb R \to \mathbb R \left| D(f) = f'(x) = 0  \right.\right\}.
> $$
> $K$ is a vector subspace of $C^1$ (you might want to convince yourself that this is true using @thm-vector-subspace, but you don't have to for this exercise).
> What is $\dim K$?

:::

## The notion of isomorphism

We have identified one property that $P_1$ and $\mathbb R^2$ have in common: they have the same dimension.
There is a more general notion of "sameness" that is true for these two vector spaces.
We will introduce the word for it here, before explaining precisely what it means in the following section.

For the purposes of this book, $V$ and $W$ are *isomorphic* if they are "the same" once we fix some piece of data.

In mathematics more generally, if two objects are isomorphic, it means that they "behave identically" but might need an extra piece of information to actually identify one with the other.
The full notion is extremely broad.[^isomorphism-link]

[^isomorphism-link]: [https://en.wikipedia.org/wiki/Isomorphism](https://en.wikipedia.org/wiki/Isomorphism)

## Isomorphism to $\mathbb R^n$ (given a basis)

Recall @thm-list-spaces, that $\mathbb R^n$ is the set of lists of length $n$, where the elements of the list are real numbers.
Recall that $\mathbb R^n$ is a vector space, and notice that we can demonstrate at least one basis for $\mathbb R^n$:

$$
\begin{aligned}
&(1,0,0, \cdots ,0,0) ,\\
&(0,1,0, \cdots ,0,0), \\
&\quad \vdots  \\
&(0,0, \cdots ,0,1,0), \\
&(0,0,0, \cdots ,0,1)
\end{aligned}
$$

Notice that this basis has exactly $n$ elements.

Now, suppose we have another vector space $V$ over $\mathbb R$ with a basis of $n$ elements $[b_1 , \ldots , b_n]$.
Given a vector $v \in V$, there is a unique list of $n$ real numbers $v_1 , \ldots , v_n$ such that
$$
v = v_1 b_1 + v_2 b_2 + \cdots + v_n b_n.
$$

As an example, consider $P_1$ with basis $[x+1,x-1]$.
The vector $12 \pi x - 2025$ can only be written in one way relative to this basis:
$$
12 \pi x - 2025 = \left ( \frac{12 \pi - 2025}{2} \right )(x+1) +  \left ( \frac{12 \pi + 2025}{2} \right )(x-1) .
$$

Once we choose a basis, every vector can be "identified" with its list of cooeficients relative to that basis.
In the example above, we have
$$
12 \pi x - 2025 \quad ''='' \quad \left ( \frac{12 \pi - 2025}{2} , \frac{12 \pi + 2025}{2} \right ).
$$
We put the equals sign in quotes here because these two vectors are not identical (they're not even in the same vector space), but once we choose a basis, we can think of the left as the right.

::: {#exr-standard-basis-p1}
> Choose a different basis for $P_1$ and write the coordinates of the vector $12 \pi x - 2025$ relative to that basis.
:::

::: {#exr-coords-to-vec}
> Choose any basis you want for $P_2$.
> 
> The set $W:=\Span\{ (1 , 0 , 0 , 0) , (0 , 0 , 1 , 0), (0 , 0 , 0 , 1), (1 , 0 , 1 , 0)\} \subset \mathbb R^4$ is a vector subspace (But you shouldn't believe me! You can check for yourself!).
> Choose a basis for this subspace.
> 
> Using your choses bases, what vector is $(1,2,3) \in \mathbb R^3$ when interpreted as coordinates for a vector in $P_3$?
> How about when interpreted as coordinates for a vector in $W$?
:::

To make this notion precise, we notice that there is a function (which depends on the basis we choose for $P_1$) which we'll name $C : P_1 \to \mathbb R^2$.
It turns out that this function is a linear function between vector spaces.

In particular, adding in $P_1$ and converting to coordinates, adding, and converting back to a polynomial produce the same value:

$$
( a x + b ) + (c x + d) = (a + c) x + (b + d)
$$
and
$$
\begin{aligned}
\underbrace{\left ( \frac{a +b}{2} , \frac{a-b}{2} \right )}_{C(ax + b)} + \underbrace{\left ( \frac{c +d}{2} , \frac{c-d}{2} \right )}_{C(cx + d)} 
=& \left ( \frac{a +b + c +d}{2} , \frac{a + c - (b+d) }{2} \right ) \\
C^{-1} \left ( \frac{a +b + c +d}{2} , \frac{a + c - (b+d) }{2} \right ) 
=&  \frac{a +b + c +d}{2}(x+1) +  \frac{a + c - (b+d) }{2} (x-1) \\
=&  (a + c) x + (b + d)
\end{aligned}
$$
where $C^{-1}$ is the inverse function.

A similar property holds for scalar multiplication.

It is perhaps clear that these properties are not unique to $P_1$ and $\mathbb R^2$, nor to the basis we chose for $P_1$ in the above example.
Generalizing this example, we arrive at the following theorem.

::: {#thm-isomorphism-up-to-dim}

Let $V$ be a finite dimensional vector space over the field $F$.
Let $n = \dim V$, and let $B$ be a choice of basis for $V$.
Then the function $V \to F^n$ taking a vector to its list of coordinates relative to $B$ is a linear function which is injective and surjective.
(That is, the function is an isomorphism of vector spaces.)

:::

::: {#exr-basis-nonindependence}
> Consider two bases $B_1 := [x^2 , x , 1]$ and $B_2 := [x^2 , x , -1]$ for $P_2$.
>
> Consider the vector $(17, 5 , 2) \in \mathbb R^3$.
> What vector in $P_2$ does this represent relative to $B_1$?
> What about relative to $B_2$?
:::

What this theorem says is that, once we choose a basis for $P_1$, we can treat it *the same as* $\mathbb R^2$.
We can work with polynomials or coordinates in $\mathbb R^2$ and we will get the same results.
The same thing is true for all vector spaces over $\mathbb R$ with dimension 2.
This fact is generalized in the following corollary:

::: {#cor-vspace-dim}

Let $V$ and $W$ be two vector spaces over the same field of scalars $F$.
If $\dim V$ and $\dim W$ are finite and equal, then $V$ and $W$ are "the same" (isomorphic) once we choose bases for each of them.

:::

The proof of this fact is that, since they have the same dimension (call it $n$), then both $V$ and $W$ can be thought of as lists of coordinates in $F^n$.

<!-- 
::: {#fig-coordinate-add}

```{.tikz}
%%| filename: coordinate-add
%%| image-width: 50%

\usetikzlibrary{matrix}
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em] {
     F_t(x) & F(x) \\
     A_t & A \\};
  \path[-stealth]
    (m-1-1) edge node [left] {$\mathcal{B}_X$} (m-2-1)
            edge [double] node [below] {$\mathcal{B}_t$} (m-1-2)
    (m-2-1.east|-m-2-2) edge node [below] {$\mathcal{B}_T$} node [above] {$\exists$} (m-2-2)
    (m-1-2) edge node [right] {$\mathcal{B}_T$} (m-2-2)
            edge [dashed,-] (m-2-1);
\end{tikzpicture}
```
::: -->


## Basis dependence

[TODO]


