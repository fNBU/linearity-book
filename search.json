[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linearity",
    "section": "",
    "text": "What are these notes?\nThese are notes for a first course in Linear Algebra.\n\nThe PDF version of these notes can be fount at https://fnbu.pw/linearity-book/Linearity.pdf.",
    "crumbs": [
      "What are these notes?"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Why publish a new set of Linear Algebra notes?\nThis section discusses why these notes exist. Students may skip this section.\nLinear Algebra, like Calculus, is one of the math subjects with the most textbooks, so it’s reasonable to ask why a new set of notes is needed. Plainly, I looked at the six open-access books on the subject on the AIMath website and found that none of them were fit for my purpose (detailed below).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#the-perspective-of-these-notes",
    "href": "preface.html#the-perspective-of-these-notes",
    "title": "Preface",
    "section": "The perspective of these notes",
    "text": "The perspective of these notes\nThese notes are constructed to vindicate the following objectives:\n\nIt is morally right for course materials to be free. Few existing books on this subject in English satisfy this criterion (eg Hefferon (2022) is GFDL or CC BY-SA 3.0 US, Treil (2024) is CC BY-NC-ND 3.0 with source unavailable). This book, and the source used to generate it are freely available with a permissive license.\nIn practice, scientists, engineers, etc. need to be able to recognize linearity so that they may choose the correct solution techniques. They also need to understand why linear problems are preferable to non-linear ones so that they might try to massage their current problem into a linear one.\nWhen we say “solution techniques” as above, 99% of the time we mean software packages. Mathematicians and physicists teach linear algebra techniques in colleges and universities, and emphasize by-hand solution techniques for historical and cultural reasons. Most working people who encounter such problems do not use such techniques, they recognize that their problem is linear and offload the problem to a software package. Mathematicians and physicists generally get a second pass at learning linear algebra in a more theory-heavy context (at the very least when learning modules), and so do not need that approach in a first course.\nThe usefulness of linear algebra techniques stems wholly from the homomorphism property of linear maps:\n\\[ L(a V + b W) = a L(V) + b L(W) \\]\nNo introductory, open-access, English language books on the topic that I am aware of motivate the study of the subject with this point. They traditionally begin with coordinate geometry or solving systems of linear equations. It is a very mathematicians’ way of thinking to motivate study of a topic by identifying a class of equations and asking “How do we solve them? What properties do they have?” This is not a way of thinking that is useful for people encountering linearity in the wild.\nMany advanced undergraduate books like Axler (2024) begin with the definition of a vector space.\nIt is also a very mathematician’s way of thinking to begin with a definition of a set of objects which will be mapped into or out of (viz. most treatments of Abstract Algebra). To talk about maps (which are really the objects of interest), surely we must first talk about (co)domains!\nCoordinate geometry is, at least, a class of real problems where linear techniques naturally arise, but the relevance of this as an example from “the wild” has basically vanished in the last 70 years. Today’s scientists and engineers are more likely to encounter linearity in optimization, data science, machine learning, or numerical PDEs.\n\nSo, for these reasons, I set out to write my own course notes.\n\n\n\n\nAxler, Sheldon. 2024. Linear Algebra Done Right. Undergraduate Texts in Mathematics. Springer, Cham. https://doi.org/10.1007/978-3-031-41026-0.\n\n\nHefferon, Jim. 2022. Linear Algebra. Fourth Ed. https://hefferon.net/linearalgebra/.\n\n\nTreil, Sergei. 2024. Linear Algebra Done Wrong. https://sites.google.com/a/brown.edu/sergei-treil-homepage/linear-algebra-done-wrong.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "what-is-linearity.html",
    "href": "what-is-linearity.html",
    "title": "1  What is Linearity?",
    "section": "",
    "text": "Two properties\nThe function \\(C : \\mathbb R \\to \\mathbb R\\) given by\n\\[ C(r) := 2 \\pi r \\]\ncomputes the circumference of a circle, given its radius.\nThe circumference of a circle has a couple nice properties. First, the circumference of a circle of radius 14 is twice the circumference of a circle of radius 7:\n\\[\n\\begin{aligned}\nC(7) &= 2 \\pi (7) = 14 \\pi   \\\\\nC(14) &= 2 \\pi (14) = 28 \\pi   \n\\end{aligned}\n\\]\nThis holds in general; multiplying the radius of a circle by \\(k\\) also changes the circumference by a factor of \\(k\\):\n\\[ C(k r) = k C(r) . \\]\nFurthermore, adding any amount to the radius increases the circumference in a predictable way:\n\\[ C(r_1 + r_2) = C(r_1) + C(r_2). \\]\nIt’s a bit remarkable that these two properties hold not just for circles; scaling any shape in the plane (with circumference \\(c\\)) by a factor of \\(k\\) multiplies its circumference by \\(k\\) (its new circumference is \\(kc\\)), and increasing the scale by a constant \\(s\\) increases its circumference by \\(sc\\).\nWhat other functions have the following two properties?\n\\[\n\\begin{aligned}\n\\text{Property 1:} \\quad & f(a x )= a f(x) \\\\\n\\text{Property 2:} \\quad & f( x + y ) = f(x) + f(y)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Linearity?</span>"
    ]
  },
  {
    "objectID": "what-is-linearity.html#two-properties",
    "href": "what-is-linearity.html#two-properties",
    "title": "1  What is Linearity?",
    "section": "",
    "text": "Exercise 1.1 Demonstrate that the area of a circle as a function of its radius does not satisfy properties 1 and 2.\n\n\nExercise 1.2 Can you think of any shape in the plane whose area (as a function of scaling) satisfies properties 1 and 2? If yes, which? If no, why not?\n\n\nExercise 1.3 Can you think of any shape in the 3-dimensional space whose volume (as a function of scaling) satisfies properties 1 and 2? If yes, which? If no, why not?\n\n\nClassification of functions satisfying Properties 1 and 2\nSo far, we have seen that some functions satisfy properties 1 and 2, and others do not.\n\nFunctions that do and don’t satisfy properties 1 and 2\n\n\n\n\n\n\ndo\ndon’t\n\n\n\n\ncircumference of a circle as a function of radius\narea of a circle as a function of radius\n\n\ncircumference of any shape in the plane as a function of scale\narea of any shape in the plane as a function of scale\n\n\n\nvolume of any shape in 3-dimensions as a function of scale\n\n\n\n\nWhy does scaling satisfy properties 1 & 2 for any shape, not just circles?\nWe asserted above that property 1 is not just satisfied by circles (when you scale the radius) but is satisfied by all curves. Why is this the case?\nWhat do we mean by scaling a figure in the plane by a factor of 2? Well, a reasonable answer is to say that a figure is a set of points and each point has an \\(x\\) and \\(y\\) coordinate. For example, the circle of radius \\(r\\) is the set of points with coordinates given by\n\\[\n\\begin{aligned}\nx =& r \\cos \\theta \\\\\ny =& r \\sin \\theta\n\\end{aligned}\n\\]\nwhere \\(\\theta \\in [0,2\\pi]\\).\n\n\n\n\n\n\nNotation\n\n\n\nWe introduced a symbol above: \\(\\in\\). This literally means “in” or “is in” depending on context.\nFor example, \\(\\pi \\in \\mathbb R\\) means “pi is in the real numbers”.\nSometimes we will want to list the set first, like \\(\\mathbb R \\ni \\pi\\). In this case we read it as “the real numbers contain pi.”\n\n\n\n\n\n\n\n\nNotation\n\n\n\nWhen we want to refer to a set, we will often use notation like the following:\n\\[\n\\left\\{ (x,y) \\in \\mathbb R^2 \\left| \\begin{aligned} x =& r \\cos \\theta \\\\ y =& r \\sin \\theta \\end{aligned} \\quad, \\theta \\in [0,2\\pi] \\right. \\right\\}.\n\\]\nIn general, this notation has the form\n\\[\nB=\\left\\{ f(x_1,x_2,...) \\in A \\left| \\begin{aligned} & \\text{ constraints involving }  \\\\ & x_1,x_2,... \\end{aligned} \\right. \\right\\}.\n\\]\nThe process for constructing the set \\(B\\) is the following:\n\nFind all the \\(x_i\\) that satisfy the constraints to the right of the \\(\\mid\\) symbol.\nPlug all the \\(x_i\\) you found in the previous step into the function \\(f\\).\nThe function \\(f\\) produces things in \\(A\\), and so the set of all the things you produced in the last step is a collection of some (but not necessarily all) of the things in \\(A\\).\nThis is the set \\(B\\).\n\n\n\nConsider the point \\((4,2)\\) in the plane. We can think of this point as the “sum” of its \\(x\\) and \\(y\\) coordinates:\n\\[\n(4,2) = (4,0) + (0,2).\n\\]\nTo scale this point by a factor of 2, it seems reasonable to multiply both coordinates by 2:\n\\[\nS_2((4,2)) = (8,4).\n\\]\nNotice that the function \\(S_2 : \\mathbb R^2 \\to \\mathbb R^2\\) “multiply coordinates by 2” has properties 1 and 2.\n\nExercise 1.4 Verify this.\n\n\nExercise 1.5 Check that applying \\(S_2\\) to a circle of radius \\(r\\) produces a circle of radius \\(2r\\).\n\nNow, for any curve \\(\\gamma : [a,b] \\to \\mathbb R^2\\) given by\n\\[\n\\gamma(t) = (x(t) , y(t)),\n\\]\nits length can be computed by\n\\[\n\\int _{a}^{b}{\\sqrt {x'(t)^{2}+y'(t)^{2}}} \\quad dt.\n\\]\n(You may have seen this in multivariable calculus or physics.) Consider the composition\n\\[\n[a,b] \\xrightarrow{\\quad \\gamma \\quad} \\mathbb R^2 \\xrightarrow{\\quad S_2 \\quad} \\mathbb R^2\n\\]\nThe formula is\n\\[\n( S_2 \\circ \\gamma )(x) =  ( 2 x(t) , 2 y(t) )\n\\]\nand the length is given by\n\\[\n\\begin{aligned}\n&\\int _{a}^{b}{\\sqrt {\\left(\\left[2x(t)\\right]'\\right)^{2}+\\left(\\left[2y(t)\\right]'\\right)^{2}}} \\quad dt \\\\\n=&\\int _{a}^{b}{\\sqrt {\\left(2x'(t)\\right)^{2}+\\left(2y'(t)\\right)^{2}}} \\quad dt \\\\\n=&\\int _{a}^{b}{\\sqrt {4\\left(x'(t)\\right)^{2}+4\\left(y'(t)\\right)^{2}}} \\quad dt \\\\\n=&\\int _{a}^{b}{2\\sqrt {\\left(x'(t)\\right)^{2}+\\left(y'(t)\\right)^{2}}} \\quad dt \\\\\n=&2\\int _{a}^{b}{\\sqrt {\\left(x'(t)\\right)^{2}+\\left(y'(t)\\right)^{2}}} \\quad dt\n\\end{aligned}\n\\]\nNotice that what we obtain on the last line is exactly twice the length of the curve \\(\\gamma\\). Convince yourself that there is nothing special about the number 2 here; if we had replaced \\(S_2\\) by \\(S_{17}\\), then we would have obtained 17 times the length of \\(\\gamma\\) in the last line.\n\n\nMore functions which break up over “sums”\n\nDifferentiation\nNow consider how we compute the derivative of a function like the following:\n\\[\n\\begin{aligned}\n\\frac{d}{dx} \\left[ 2x^2 + x \\right] &= \\frac{d}{dx} \\left[ 2x^2 \\right] + \\frac{d}{dx} \\left[ x \\right] \\\\\n&= 2\\frac{d}{dx} \\left[ x^2 \\right] + \\frac{d}{dx} \\left[ x \\right] \\\\\n&= 4x + 1\n\\end{aligned}\n\\]\nIf we let \\(D : \\text{FUNCTIONS} \\to \\text{FUNCTIONS}\\) be the operation of taking a derivative, then in the first line we used\n\\[ D[f_1(x) + f_2(x)] = D[f_1(x)] + D[f_2(x)] \\]\nand in the second line we used the fact that, when \\(k\\) is constant,\n\\[ D[k f(x)] = k D[f(x)]. \\]\nThus, \\(D\\) (that is, differentiation) satisfies properties 1 and 2. (Although you should probably be uncomfortable that we wrote \\(D : \\text{FUNCTIONS} \\to \\text{FUNCTIONS}\\) above. What is the set \\(\\text{FUNCTIONS}\\)? Are all functions differentiable? We will address this later. For now, it suffices to replace \\(\\text{FUNCTIONS}\\) above with \\(P_n\\), the set of all polynomials of degree at most \\(n\\). In fact, it is the case that \\(D : P_n \\to P_{n-1}\\) if \\(n\\geq 1\\).)\n\n\nDefinite integration\nFix an interval \\([a,b]\\) and consider \\[\nI(f) := \\int_a^b f(x) \\quad dx.\n\\]\n\nExercise 1.6 Check that \\(I\\) has properties 1 and 2.\n\n\n\n\nWhy are properties 1 and 2 useful?\nWhy is it useful that \\(D\\) satisfies properties 1 and 2? It allows us to compute derivatives of complicated expressions like \\(2x^2 + x\\) if we only know the computation on some simple parts of the expression. Knowing the derivative of \\(x^2\\) and \\(x\\) is all that is needed.\nSimilarly, if we know the circumference of a shape in the plane at one scale, we can compute its circumference at all scales using property 1.\nNot all functions are linear, but if a function is linear, it is much easier to compute with.",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Linearity?</span>"
    ]
  },
  {
    "objectID": "what-is-linearity.html#linear-functions",
    "href": "what-is-linearity.html#linear-functions",
    "title": "1  What is Linearity?",
    "section": "Linear functions",
    "text": "Linear functions\n\nDefinition 1.1 (linear function) A function satisfying properties 1 and 2 is called linear.\n\n\n\n\n\n\n\nWarning\n\n\n\nWe use the term “linear” for these functions, but we also use the word “line” for graphs in the plane with formula \\(f(x) = m x + b\\). This term is overloaded and means different things in these two contexts.\n\n\n\nExercise 1.7 Show that \\(f(x) = m x + b\\) is only a linear function when \\(b = 0\\).",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Linearity?</span>"
    ]
  },
  {
    "objectID": "the-notion-of-a-vector-space.html",
    "href": "the-notion-of-a-vector-space.html",
    "title": "2  The notion of a Vector Space",
    "section": "",
    "text": "Definition\nIf \\(L : A \\to B\\) is linear, what must be true about \\(A\\) and \\(B\\)?\nLet’s go back to the definition of a linear function. A function is linear if and only if it satisfies the following two properties:\n\\[\n\\begin{aligned}\n\\text{Property 1:} \\quad & L(a x )= a L(x) \\\\\n\\text{Property 2:} \\quad & L( x + y ) = L(x) + L(y).\n\\end{aligned}\n\\]\nLet’s list a few things that must be true to arrive at these expressions:\nThere are perhaps some more properties that would be nice, and that are true about all the domains and codomains of linear functions we have seen so far:\nEventually, mathematicians (who were working with linear functions intuitively) worked out the minimal set of facts that one needs about the domain and codomain of a linear function for everything to be coherent. Here it is\nNote that the notation \\(+ \\colon V \\times V \\to V\\) implies that \\(V\\) is, in particular, closed under addition (since \\(V\\) is the codomain of \\(+\\); addition cannot by definition return an element not in \\(V\\)). Similarly we can conclude that \\(V\\) must be closed under scalar multiplication.\nAll of the properties above hold (with the field of scalars \\(F\\) set to \\(\\mathbb R\\)) for all the spaces we have used so far as the domain and codomain of a linear function. (You may want to check this for yourself quickly. Most of these facts obviously hold for polynomials, functions, points in \\(\\mathbb R^2\\), and \\(\\mathbb R\\) itself. However, one must still check all of them before using a set as the domain or codomain of a linear function.)\nNot all sets of mathematical objects satisfy these properties, though. An example is the set of points on the sphere. It turns out that there is no way (although the proof will have to wait for a future chapter) to turn that set into a vector space.\nThe way you should think about this definition is the following:\nAn example of the kind of problem you can run into is the following: Let \\(L:A \\to B\\) be linear and suppose \\(A\\) is a vector space as defined above but that \\(B\\) fails to have an additive identity (that is, there is no zero vector in \\(B\\)). We can compute\n\\[\nL(0) = 0L(0) = 0 (?) = 0\n\\] where the \\(?\\) stands for whatever \\(L(0)\\) maps to. Notice that in the end, it doesn’t matter because we conclude that \\(L(0) = 0 \\in B\\). But this is nonsensical since we assumed there was no zero vector in \\(B\\).",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The notion of a Vector Space</span>"
    ]
  },
  {
    "objectID": "the-notion-of-a-vector-space.html#definition",
    "href": "the-notion-of-a-vector-space.html#definition",
    "title": "2  The notion of a Vector Space",
    "section": "",
    "text": "Definition 2.1 A vector space is a set \\(V\\) of vectors and a set \\(F\\) of scalars that satisfy the following properties\n\nthe set of scalars \\(F\\) is a field (see the next section for more information on what we mean by field),\nthere is a function \\(+ \\colon V \\times V \\to V\\), called vector addition that\n\nis associative: \\((X+Y) + Z = X + (Y + Z)\\),\nis commutative: \\(X+Y = Y+X\\),\nhas a \\(0\\) (an additive identity) which means that \\(0\\) satisfies \\(0 + X = X\\) for all \\(X \\in V\\),\nhas negatives (additive inverses) which means that for each \\(X \\in V\\) there is an element \\(Y \\in V\\) such that \\(X + Y = 0\\) (one can prove that there is only one inverse of \\(X\\), and this is usually written \\(-X\\)),\n\nthere is a function \\(\\cdot \\colon F \\times V \\to V\\), called scalar multiplication such that\n\nscalar multiplication associates with field multiplication (that is, \\(\\forall a , b \\in F, X \\in V\\) it’s true that \\((ab)X = a (bX)\\))\nthe multiplicative unit in \\(F\\) is a unit for scalar multiplication (that is, \\(\\forall X \\in V\\) we have \\(1 X = X\\))\n\nthe following distributive laws hold:\n\n\\((a+b) X = aX + bX , \\quad \\forall a, b\\in F , X \\in V\\)\n\\(a(X+Y) = aX + aY , \\quad \\forall a \\in F ,  X,Y \\in V\\)\n\n\n\n\n\n\n\n\n\nNotation\n\n\n\nAbove, we used the symbol \\(\\forall\\). This is a symbol that mathematicians use that literally just means “for all”.\nSo, as an example \\(x \\in P_3 , \\forall x \\in P_2\\) is read as “for all x in the set of second degree polynomials, x is in the set of third degree polynomials”, which is just a wordy way of saying that all second degree polynomials are third degree polynomials.\n\n\n\n\n\n\n\nwe are allowed to use a set \\(A\\) satisfying all of the properties in Definition 2.1 as the domain or codomain of a linear function, but\na set \\(A\\) that fails to satisfy any of the properties above can never be used as the domain or codomain of a linear function. If you tried to use it in this way, you would eventually run into statements that make no sense.1\n\n\n\n\nExercise 2.1 Check that \\(\\mathbb R\\) forms a vector space (the set of scalars is \\(\\mathbb R\\)).\n\n\nExercise 2.2 Check that \\(\\mathbb R^2\\) is a vector space.",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The notion of a Vector Space</span>"
    ]
  },
  {
    "objectID": "the-notion-of-a-vector-space.html#scalars",
    "href": "the-notion-of-a-vector-space.html#scalars",
    "title": "2  The notion of a Vector Space",
    "section": "Scalars",
    "text": "Scalars\nIn the above definition, we said that the scalars for a vector space must come from a field. What does this mean?\nFor the purposes of this text, our field will always be \\(\\mathbb R\\) or the field of complex numbers:\n\\[\n\\mathbb C := \\{ a + b i \\mid a,b \\in \\mathbb R \\}\n\\]\nwhere the symbol \\(i\\) has the property that \\(i^2 = -1\\) (and so we sometimes write it as \\(\\sqrt{-1}\\)). These two sets are the set of scalars for most vector spaces found in applications in the wild.\n\nExercise 2.3 Check that \\(\\mathbb C\\) can be thought of as a vector space with field of scalars \\(\\mathbb R\\).\n\n\nDefinition 2.2 Let \\(P_n\\) be the set (which we mentioned informally in the previous chapter)\n\\[\n\\left\\{ a_{n} x^n + a_{n-1} x^n + \\cdots + a_1 x + a_0 \\mid a_n , a_{n-1}, \\ldots , a_1 , a_0 \\in \\mathbb R \\right\\}.\n\\]\nThis is called the set of polynomials of degree \\(n\\).\n\n\nExercise 2.4 Is \\(P_n\\) a vector space for each \\(n \\in \\mathbb N\\)?\n\nFrom the previous two examples, you might notice that the definitions\n\\[\n\\mathbb C := \\{ a + b i \\mid a,b \\in \\mathbb R \\}\n\\]\nand\n\\[\nP_n := \\left\\{ a_{n} x^n + a_{n-1} x^n + \\cdots + a_1 x + a_0 \\mid a_n , a_{n-1}, \\ldots , a_1 , a_0 \\in \\mathbb R \\right\\}\n\\]\nlook similar, and they give you a hint about how to think of these as vector spaces; in both cases, they are constructed as the sum of things with coefficients in \\(\\mathbb R\\). We will investigate this further in the next chapter.\n\nFields\nIn case you are interested, the full definition of a field can be found on Wikipedia. A field that you already know about, but that is not \\(\\mathbb R\\) or \\(\\mathbb C\\) is the field of rational numbers:\n\\[\n\\mathbb Q := \\left\\{ \\left. \\frac{a}{b}  \\right| a,b \\in \\mathbb Z ,  b \\ne 0  \\right\\}.\n\\]\nThere are esoteric examples of fields, and also algebraic structures stranger than fields. If this is interesting to you, you might try to take a course in Abstract Algebra. I learned this topic from Liz Stanhope and Dummit and Foote (2004).\nIn computing, real numbers (which do form a field) are often represented by 32 or 64 bit floating point numbers. It is perhaps interesting to know that floating point numbers (in any number of bits) (which are encountered often in the wild) do not form a field because addition of floating point numbers is not associative.",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The notion of a Vector Space</span>"
    ]
  },
  {
    "objectID": "the-notion-of-a-vector-space.html#subspaces",
    "href": "the-notion-of-a-vector-space.html#subspaces",
    "title": "2  The notion of a Vector Space",
    "section": "Subspaces",
    "text": "Subspaces\nWe have seen in this chapter that if you want to do something with a linear function, you better check first that its domain and codomain are vector spaces. It’s quite a lengthy process to verify all the properties in Definition 2.1, even if in many cases it is not difficult to verify each one individually. We are in need of machines that allow us to know that something is a vector space more easily. That is what this and the next section are about.\nHow do we know that a set \\(W \\subset V\\) is a vector space?\n\n\n\n\n\n\nNotation\n\n\n\nAbove, we used the symbol \\(\\subset\\). This just means “is a sub set of”, for example \\(\\mathbb Z \\subset \\mathbb Q\\) means “the set of integers is a subset of the set of rational numbers.”\nWhen we say “A is a subset of B” or, equivalently write \\(A \\subset B\\), we mean that everything in \\(A\\) is also in \\(B\\).\nWe will sometimes write \\(A \\supset B\\) to mean that \\(B\\) is a subset of \\(A\\) or that “A has B as a subset.”\n\n\nWe’ve already seen some examples of this. For example, we know that differentiable functions form a vector space, and also the set \\(P_n\\) forms a vector space. Since every polynomial is differentiable, it follows that \\(P_n \\subset \\{ \\text{differentiable functions}\\}\\). So \\(P_n\\) is an example of a subset of a vector space that turns out to be a vector space itself.\nLet’s introduce some terminology for this situation.\n\nDefinition 2.3 A subset \\(W\\) of a vector space \\(V\\) is called a (vector) subspace if \\(W\\) is itself a vector space (with field of scalars \\(F\\), addition, multiplication, zero vector inherited from \\(V\\)).\n\nWe have already seen an example of a subspace above. A nonexample is the set of points in the plane at distance 1 from the origin. This set doesn’t satisfy the closure assumptions of Definition 2.1.\nIt is a bit remarkable (although not difficult to prove) that we actually only need to check the closure assumptions. The following theorem encodes this fact.\n\nTheorem 2.1 A subset \\(W\\) of a vector space \\(V\\) is itself a vector space (with field of scalars \\(F\\), addition, multiplication, zero vector inherited from \\(V\\)) iff the following two properties hold:\n\n\\(W\\) is closed under scalar multiplication: \\(aX \\in W \\quad \\forall a \\in F, X \\in W\\)\n\\(W\\) is closed under vector addition: \\(X+Y \\in W \\quad \\forall X,Y \\in W\\)\n\n\nWe won’t do the proof here (you can go to e.g. Hefferon (2022) to see it), but we will sketch it.\nSince the scalars, and the operations \\(\\cdot , +\\) are inherited from the ambient space \\(V\\), they are known to satisfy associativity, commutativity, existence of inverses and zeros, etc (because we had to prove those things when we showed that \\(V\\) was a vector space). However, in Definition 2.1, we wrote \\(+ \\colon V \\times V \\to V\\). That is, \\(+\\) takes two vectors in \\(V\\) and returns a vector in \\(V\\). Since \\(W \\subset V\\), we know that we can use elements of \\(W\\) as inputs to \\(+\\). That is, we know \\(+ \\colon W \\times W \\to V\\), but we cannot know addition of vectors always lands in the subset \\(W\\). The first assumption above tells us this. Similarly for scalar multiplication.\nFor the proof in the opposite direction, notice that for \\(W\\) to be a vector space we must have \\(+ \\colon W \\times W \\to W\\). If addition is not closed, then \\(+\\) has the wrong codomain, and \\(W\\) isn’t a vector space. Similarly if \\(W\\) is not closed under scalar multiplication.\n\nEasier subspaces\nNotice that we can use Theorem 2.1 to quite easily prove (assuming we know differentiable functions form a vector space) that \\(P_n\\) is a vector space. We can also conclude easily that, for example,\n\\[\n\\{ a \\sin x + b \\cos x \\mid a,b \\in \\mathbb R \\}\n\\]\nis a vector space.\n\nExercise 2.5 Is the set of arrows that are either fully horizontal or fully vertical a vector subspace of the vector space of arrows in 2D? Why?\n\n\nExercise 2.6 Consider the set of monomials\n\\[\n\\{ 1 , x , x^2 , x^3 , \\ldots  \\} .\n\\]\nIf I pick any two of these (say \\(x^k\\) and \\(x^l\\) with \\(k \\ne l\\), eg \\(x^{17}\\) and \\(x^{2025}\\)) and form the set\n\\[\n\\{ a x^k + b x^l \\mid a,b \\in \\mathbb R \\} ,\n\\]\nis this a vector subspace of the space of differentiable functions? Why?",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The notion of a Vector Space</span>"
    ]
  },
  {
    "objectID": "the-notion-of-a-vector-space.html#the-space-of-lists-of-length-n",
    "href": "the-notion-of-a-vector-space.html#the-space-of-lists-of-length-n",
    "title": "2  The notion of a Vector Space",
    "section": "The space of lists of length \\(n\\)",
    "text": "The space of lists of length \\(n\\)\nWe have seen that the following are vector spaces: \\(\\mathbb R\\) over \\(\\mathbb R\\), \\(\\mathbb R^2\\) over \\(\\mathbb R\\), \\(\\mathbb C\\) over \\(\\mathbb C\\).\nThere is a very powerful theorem that we can prove to generate many vector spaces that are of a similar form.\n\nTheorem 2.2 Let \\(F\\) be any field. Define the following set\n\\[\nF^n := \\{ (a_1 , a_2 , \\cdots , a_n) \\mid a_i \\in F \\}.\n\\]\nThis is the space of lists of length \\(n\\), with members from \\(F\\).\nLet \\(+ : F^n \\times F^n \\to F^n\\) be defined by\n\\[\n(a_1 , a_2 , \\cdots , a_n) + (b_1 , b_2 , \\cdots , b_n) = (a_1 + b_1 , a_2 + b_2 , \\cdots , a_n + b_n )\n\\]\nand let \\(\\cdot : F \\times F^n \\to F^n\\) be defined by\n\\[\nx (a_1 , a_2 , \\cdots , a_n) = (xa_1 , xa_2 , \\cdots , xa_n).\n\\]\nThen \\(F^n\\) with these two operations (and field of scalars \\(F\\)) is a vector space.\n\nThe proof is a straightforward check of all the items in Definition 2.1.\nNote that \\(\\mathbb R, \\mathbb R^2 , \\mathbb C\\) can be proved to be vector spaces using this theorem.\nTo develop your intuition for what this looks like, let’s do a multiplication in \\(\\mathbb C^4\\):\n\\[\n\\begin{aligned}\n& i (1 , i , 1 + i , 2 - 3 i)\\\\\n=& (i , i^2 , i + i^2 , 2 i - 3 i^2) \\\\\n=& ( i , -1 , i -1 , 2i +3).\n\\end{aligned}\n\\]\nNote that this theorem only works if \\(F^n\\) is a vector space over \\(F\\). So, above, \\(\\mathbb C^4\\) is a vector space with scalars \\(\\mathbb C\\).\n\nExercise 2.7 Is \\(\\mathbb R^n\\) a vector subspace of \\(\\mathbb C^n\\)? Think very carefully about Definition 2.3.\n\n\n\n\n\nDummit, David S., and Richard M. Foote. 2004. Abstract Algebra. Third Ed. John Wiley & Sons, Inc., Hoboken, NJ.\n\n\nHefferon, Jim. 2022. Linear Algebra. Fourth Ed. https://hefferon.net/linearalgebra/.",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The notion of a Vector Space</span>"
    ]
  },
  {
    "objectID": "the-notion-of-a-vector-space.html#footnotes",
    "href": "the-notion-of-a-vector-space.html#footnotes",
    "title": "2  The notion of a Vector Space",
    "section": "",
    "text": "It is the case that some maps between spaces that are weaker than vector spaces have defining properties 1 and 2. The spaces are a generalization of vector spaces called modules and the maps are called module homomorphisms. If you look at the definition of module homomorphism on wikipedia, you’ll see the same two equations we used to define linear functions.↩︎",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The notion of a Vector Space</span>"
    ]
  },
  {
    "objectID": "bases.html",
    "href": "bases.html",
    "title": "3  Bases",
    "section": "",
    "text": "What is the minimal amount of information needed to unambiguously describe a linear function?\nRemember that the derivative function \\(D(f) := \\frac{df}{dx}\\) allows us to compute the derivative of, for example\n\\[\nf(x) = x^2 + 4 \\sin x\n\\]\nif we know how to compute \\(D\\) on only \\(x^2\\) and \\(\\sin x\\). In fact, knowing just how to compute \\(D\\) on these two functions and knowing that \\(D\\) is linear allows us to compute the derivative of any function of the form\n\\[\nx \\mapsto a x^2 + b \\sin x , \\quad a,b \\in \\mathbb R.\n\\]\nFrom a very small amount of information, we actually know a lot about \\(D\\). Thus, we begin this chapter straightforwardly with a question, which is the title of the next section:\nConsider the following example:\nAssume that \\(L : P_2 \\to P_0\\) is linear and that\n\\[\nL(x^2 + 3x) = 4.\n\\]\nCan we determine \\(L\\) on every element of \\(P_2\\)? That is, can we compute\n\\[\nL(a x^2 + b x + c)?\n\\]\nLet’s try our best. We would like to isolate a term that looks like \\(x^2 + 3x\\) because that is something we have information about.\n\\[\n\\begin{aligned}\nL(a x^2 + b x + c)=& L(a x^2 + b x)  + L(c) \\quad &\\text{by property 1} \\\\\n=& L\\left(a \\left[ x^2 + \\frac{b}{a} x \\right] \\right)  + L(c) \\quad &\\text{assuming } a \\ne 0 \\\\\n=& a L\\left( x^2 + \\frac{b}{a} x \\right)  + L(c) &\\quad \\text{by property 2}  \\end{aligned}\n\\]\nthen we are basically stuck. Although we can force \\(x^2\\) to show up, we can’t at the same time force \\(3x\\) to show up; forcing a coefficient of \\(1\\) on \\(x^2\\) will always effect the coefficient of \\(x\\). Furthermore, we have no information about how to deal with \\(L(c)\\).\nSo, in this case, knowing that \\(L\\) is linear and knowing its value on one input is insufficient.",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases</span>"
    ]
  },
  {
    "objectID": "bases.html#what-is-the-minimal-amount-of-information-needed-to-unambiguously-describe-a-linear-function",
    "href": "bases.html#what-is-the-minimal-amount-of-information-needed-to-unambiguously-describe-a-linear-function",
    "title": "3  Bases",
    "section": "",
    "text": "Exercise 3.1 In the example above, is it enough to know that \\[\n\\begin{aligned}\nL(x^2) =& 1 , \\\\\nL(x) =& 1 ,\n\\end{aligned}\n\\]\nto be able to determine \\(L\\) on all of \\(P_2\\)?\nWhat if we know \\[\n\\begin{aligned}\nL(x^2) =& 1 , \\\\\nL(x) =& 1 ,\\\\\nL(1) =& -2\n\\end{aligned}?\n\\]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases</span>"
    ]
  },
  {
    "objectID": "bases.html#writing-vectors-relative-to-a-set-of-vectors",
    "href": "bases.html#writing-vectors-relative-to-a-set-of-vectors",
    "title": "3  Bases",
    "section": "Writing vectors relative to a set of vectors",
    "text": "Writing vectors relative to a set of vectors\nLet \\(X,Y \\in \\mathbb R^2\\) be given by \\(X:= (1,1)\\) and \\(Y:= (1,-1)\\). Suppose \\(L : \\mathbb R^2 \\to \\mathbb R\\) is linear and also that we know \\[\n\\begin{aligned}\nL(X) =&  1, \\\\\nL(Y) =&  -2\n\\end{aligned}.\n\\]\nCan we compute \\(L\\) on the vector \\((2,3)\\)? If the answer to this question were yes then there would have to be a way to express \\((2,3)\\) in terms of \\(X\\) and \\(Y\\) (since those are the only two values we know anything about). That is, there would be \\(a,b \\in \\mathbb R\\) such that\n\\[\n(2,3) = a X + b Y = a (1,1) + b(1 , -1) = (a , a) + ( b , -b ) = (a + b , a - b).\n\\]\nSo, \\(L\\) can be computed on \\((2,3)\\) if and only if the following system has a solution:\n\\[\n\\begin{aligned}\n2 =& a + b ,\\\\\n3 =& a - b\n\\end{aligned}.\n\\]\nWe can solve this system (solving for, say, \\(a\\) in the first and substituting the value in the second). We find that there is only one solution: \\(a = \\frac{5}{2} , b= -\\frac{1}{2}\\).\nUsing this information, how do we compute \\(L(2,3)\\)? Well, now we know that\n\\[\n(2,3) = \\frac{5}{2} (1 , 1) + \\left( -\\frac{1}{2} \\right) (1,-1)\n\\]\nso let’s just apply the function \\(L\\) to both sides of this equation:\n\\[\n\\begin{aligned}\nL(2,3) =& L \\left[ \\frac{5}{2} (1 , 1) + \\left( -\\frac{1}{2} \\right) (1,-1) \\right] \\\\\n=& L \\left[ \\frac{5}{2} (1 , 1) \\right] + L \\left[ \\left( -\\frac{1}{2} \\right) (1,-1) \\right] \\\\\n=& \\frac{5}{2} L   (1 , 1) + \\left( -\\frac{1}{2} \\right) L  (1,-1) \\\\\n=& \\frac{5}{2} 1 + \\left( -\\frac{1}{2} \\right) (-2) \\\\\n=& \\frac{5}{2}  + 1\\\\\n=& \\frac{7}{2}\n\\end{aligned}\n\\]\n\nExercise 3.2 For each \\(=\\)-symbol in the computation above, write the assumption, property, or rule that tells us we are allowed to conclude the left hand side is equal to the right hand side.\n\nThe above computation shows that the answer to our question is yes, this amount of information about \\(L\\) is sufficient to compute \\(L(2,3)\\). Notice that this is the case even though we were not given the formula for \\(L\\).\nYou may want to convince yourself that the vector \\((2,3)\\) above was not special; given the information we have about \\(L\\), we can compute \\(L(k,l)\\) for any \\((k,l) \\in \\mathbb R^2\\).\n\nExercise 3.3 If we know that \\(L : \\mathbb R^2 \\to \\mathbb R\\) is linear and that\n\\[\nL(0,1) = 1 , \\quad L(0,2) = 2,\n\\]\ncan we compute \\(L(1,2)\\)? If yes, compute it. If no, why not?\n\n\nExercise 3.4 If we know that \\(L : \\mathbb R^2 \\to \\mathbb R\\) is linear and that\n\\[\nL(0,1) = 2 , \\quad L(1,1) = 2,\n\\]\ncan we compute \\(L(1,2)\\)? If yes, compute it. If no, why not?\n\n\nLinear combinations\nThe trick to computing \\(L(2,3)\\) in the previous section was to rewrite \\((2,3)\\) in the form\n\\[\n(2,3) = a (1,1) + b(1 , -1).\n\\]\nThis is a good ansatz1 because we are able to split over the \\(+\\) and pull the \\(a\\) and \\(b\\) out (using the fact that \\(L\\) is linear). The following definition captures this pattern in general.\n\nDefinition 3.1 If \\(X_1, X_2 , \\ldots , X_n\\) is a collection of vectors in the vector space \\(V\\), then any expression of the form\n\\[\na_1 X_1 + a_2 X_2 + \\cdots + a_n X_n, \\quad a_i \\in F\n\\]\nis called a linear combination of \\(X_1, X_2 , \\ldots , X_n\\).\n\nFor some of the following exercises, we need to know about the following definition:\n\nDefinition 3.2 We use \\(C^k(\\mathbb R, \\mathbb R)\\) to denote the set of functions \\(\\mathbb R \\to \\mathbb R\\) such that, \\(\\forall f \\in C^k(\\mathbb R, \\mathbb R)\\)\n\n\\(f\\) has at least \\(k\\) derivatives and\nall of \\(f\\)’s \\(k\\) derivatives are continuous.\n\n\nSometimes we write just \\(C^k\\) rather than \\(C^k(\\mathbb R, \\mathbb R)\\) if it is implicit from the context that we are talking about functions \\(\\mathbb R \\to \\mathbb R\\).\nNotice, in particular, that \\(C^0\\) is just the set of continuous functions \\(\\mathbb R \\to \\mathbb R\\). \\(C^1\\) is the set of functions with at least one derivative which is continuous.\n\\(C^\\infty\\) is used to denote the set of functions \\(\\mathbb R \\to \\mathbb R\\) that have infinitely many derivatives, all of which are continuous (many “nice” familiar functions are in \\(C^\\infty\\), for example \\(x \\mapsto \\sin x\\), \\(x \\mapsto e^x\\) and any polynomial).\n\nLemma 3.1 For any \\(k\\), the set \\(C^k(\\mathbb R, \\mathbb R)\\) is a vector space over \\(\\mathbb R\\).\n\nThe proof of this lemma follows from the fact that \\(\\mathbb R\\) is a vector space over \\(\\mathbb R\\), and when we do operations on functions, we really do operations inside the codomain of the function.\n\nExercise 3.5 Is\n\\[\n\\pi \\sin x - 100 x^3\n\\]\na linear combination in \\(C^0\\)? If no, why not? If yes, what is a set of vectors in \\(C^0\\) that this is a linear combination of?\n\n\nExercise 3.6 Is\n\\[\n7 \\cos x \\sin x\n\\]\na linear combination in \\(C^0\\)? If no, why not? If yes, what is a set of vectors in \\(C^0\\) that this is a linear combination of?\n\nWe then have the following generalization of the computation we did in the example above:\n\nTheorem 3.1 Let \\(L: A \\to B\\) be linear. These two statements are equivalent:\n\nWe know the value of \\(L\\) on a set of vectors \\(X_1 ,  X_2 , \\ldots , X_n \\in A\\). That is,\n\\[\n\\begin{aligned}\nL(X_1) =& Y_1 \\\\\nL(X_2) =& Y_2 \\\\\n\\vdots & \\\\\nL(X_n) =& Y_n\n\\end{aligned}\n\\]\nfor some \\(Y_i \\in B\\).\nWe know the value of \\(L\\) on any linear combination of \\(X_1 ,  X_2 , \\ldots , X_n\\).\n\n\nThe proof \\(\\implies\\) is a straightforward application of properties 1 and 2 of linear functions. You might try to generate the justification yourself. The proof of \\(\\impliedby\\) is an application of proof by contradiction.\n\n\n\n\n\n\nNote\n\n\n\nTheorem 3.1 is not a tautology!\nThe set \\(\\{X_1 ,  X_2 , \\ldots , X_n\\}\\) is a finite set (think of \\(\\{(1,1) , (1,-1)\\}\\) which contains only 2 elements).\nThe set of linear combinations of \\(\\{X_1 ,  X_2 , \\ldots , X_n\\}\\) is infinite (if the field of scalars is infinite, which in the case of \\(\\mathbb R\\) or \\(\\mathbb C\\) it is).\nThe set of linear combinations of \\(\\{(1,1) , (1,-1)\\}\\) contains, for example, \\[\n\\begin{aligned}\n&(1,1),(2,2),(a,a) \\forall a \\in \\mathbb R ,\\\\\n&(\\pi,e),\\left(\\sqrt{2},2025^{1/3}\\right),\\\\\n&a(1,1) + b(1,-1) = (a+b,a-b) \\forall a,b \\in \\mathbb R.\n\\end{aligned}\n\\] In particular, this set is infinite.\n\n\n\n\nThe span of a set of vectors\nWe will use the words “is a linear combination of” or “can be expressed as a linear combination of” quite often, so we introduce some terminology to shorthand this.\n\nDefinition 3.3 If \\(V\\) is a vector space over a field \\(F\\), \\(X_1 , \\ldots , X_n \\in V\\), and \\(W\\) is expressible as a linear combination of \\(\\{X_1 , \\ldots , X_n \\}\\) we say that \\(W\\) is in the span of \\(\\{X_1 , \\ldots , X_n \\}\\). We can write this\n\\[\nW \\in \\Span \\{X_1 , \\ldots , X_n \\}.\n\\]\nWe also refer to the following set as the span of \\(\\{X_1 , \\ldots , X_n \\}\\): \\[\n\\Span \\{X_1 , \\ldots , X_n \\} := \\{ a_1 X_1 + \\cdots + a_n X_n \\mid a_i \\in F \\}.\n\\]\n\nSo, for example, \\((0,2025) \\in \\Span \\{(0,1),(0,6)\\}\\), but the vector \\((17,9) \\not \\in \\Span \\{(0,1),(0,6)\\}\\).\n\nExercise 3.7 Let \\((a,b) \\in \\mathbb R^2\\). Is \\((a,b) \\in \\Span \\{ (1,1) , (1,-1) \\}\\)? Why or why not?\n\n\nExercise 3.8 Let \\((a,b) \\in \\mathbb R^2\\). Is \\((a,b) \\in \\Span \\{ (1,2) , (1 ,5) , (1 , -3) \\}\\)? Why or why not?\n\nGiven the above definition, we can rephrase Theorem 3.1:\n\nTheorem 3.2 Let \\(L: A \\to B\\) be linear. These two statements are equivalent:\n\nWe know the value of \\(L\\) on a set of vectors \\(X_1 ,  X_2 , \\ldots , X_n \\in A\\).\nWe know the value of \\(L\\) on \\(\\Span \\{ X_1 ,  X_2 , \\cdots , X_n \\}\\).\n\n\nThere isn’t really anything to prove here; this is exactly Theorem 3.1 except with “any linear combination of” replaced by \\(\\Span\\), which is Definition 3.3.\n\nExercise 3.9 Is \\(\\Span \\{ 1 + x , x + x^2 \\} = P_2\\)? Why or why not?\n\n\nExercise 3.10 Show that \\(\\Span \\{ 1 + x , x + x^2, x^2 , x^3 \\} = P_3\\) by finding a representation of \\[\nax^3 + b x^2 + c x + d\n\\] (which is the form of a generic element of \\(P_3\\)) as a linear combination of \\(\\{ 1 + x , x + x^2, x^2 , x^3 \\}\\).\n\n\nExercise 3.11 Suppose we know that \\(F'(x) = f(x)\\) and \\(G'(x) = g(x)\\) (that is, \\(F,G\\) have derivatives, and we give the names \\(f,g\\) to those derivatives). Recall the definition of the operator \\(D\\) for differentiation: \\[\nD(h) := h'(x),\n\\] and that we already know \\(D : \\{ \\text{differentiable functions} \\} \\to \\{ \\text{functions}\\}\\) is linear. (In view of the notation introduced in Definition 3.2, we can now write this as \\(D: C^1 \\to C^0\\).)\nIs it true that \\[\nD\\left[F(x) + 14G(x)\\right]=f(x) + 14 g(x)?\n\\] Did you use any properties about \\(D\\) other than linearity to conclude that?\nSuppose \\(G(x) &gt; 0 , \\forall x\\). Is it true that \\[\nD\\left [ \\frac{F(x)}{G(x)} \\right] = \\frac{f(x) G(x) - F(x)g(x)}{\\left[G(x)\\right]^2}?\n\\] Did you use any properties about \\(D\\) other than linearity to conclude that?\n\n\n\nLinear independence\nSuppose we have a collection of vectors \\(\\{ X_1 , \\ldots , X_n \\} \\in \\mathbb R^2\\). We have seen examples where\n\n\\(\\{ X_1 , \\ldots , X_n \\}\\) consists of two vectors and \\(\\Span \\{ X_1 , \\ldots , X_n \\}\\) is not all of \\(\\mathbb R^2\\)\n(this happened when \\(\\{ X_1 , \\ldots , X_n \\} = \\{(0,1) , (0,2)\\}\\)),\n\\(\\{ X_1 , \\ldots , X_n \\}\\) consists of two vectors and \\(\\Span \\{ X_1 , \\ldots , X_n \\}\\) is equal to \\(\\mathbb R^2\\)\n(this happened when \\(\\{ X_1 , \\ldots , X_n \\} = \\{(1,1) , (1,-1)\\}\\)),\n\\(\\{ X_1 , \\ldots , X_n \\}\\) consists of three vectors and \\(\\Span \\{ X_1 , \\ldots , X_n \\}\\) is equal to \\(\\mathbb R^2\\)\n(this happened when \\(\\{ X_1 , \\ldots , X_n \\} = \\{(1,2) , (1 , 6) , (1,-3)\\}\\)).\n\nIn the last case, we got something extra; there were infinitely many ways of representing an arbitrary element \\((a,b) \\in \\mathbb R^2\\) as a linear combination of \\(\\{(1,2) , (1 , 6) , (1,-3)\\}\\). In the second case, we get a unique way of representing each vector as a linear combination, and in the first case we don’t even get any way to represent some vectors as a linear combination.\nFrom Definition 3.3 we now can say that the distinction between the first case and the other two is that\n\\[\n\\Span \\{(1,1) , (1,-1)\\} = \\Span \\{(1,2) , (1 , 6) , (1,-3)\\} = \\mathbb R^2\n\\]\nbut that\n\\[\n\\Span \\{(0,1) , (0,2)\\} \\ne \\mathbb R^2.\n\\]\nFrom Theorem 3.2, we know that if we know a linear function on the two vectors\n\\[\n(1,1) , (1,-1)\n\\]\nor on the three vectors\n\\[\n(1,2) , (1 , 6) , (1,-3)\n\\]\nthen we know its value on all inputs \\((a,b) \\in \\mathbb R^2\\).\nWhat is the criterion that distinguishes the second case from the last one? That is, is there a property we can measure about the set of vectors \\[\n\\{(1,2) , (1 , 6) , (1,-3)\\}\n\\] that is different when we measure it about \\[\n\\{(1,1) , (1,-1)\\}\n\\] that tells is that we get a unique linear combination in the latter case, but a nonunique linear combination in the former case? It turns out to be the following definition, and we will see why in a later part of the book.\n\nDefinition 3.4 Let \\(V\\) be a vector space and \\(\\{ X_1 , \\ldots , X_n \\} \\subset V\\). We say that the set \\(\\{ X_1 , \\ldots , X_n \\}\\) is linearly independent if, for each \\(i \\in \\{ 1 , \\ldots , n \\}\\),\n\\[\nX_i \\not \\in \\Span \\{X_1 , X_2 , \\cdots , X_{i-1} , X_{i + 1} , \\cdots , X_n\\}.\n\\]\nThis is another way of saying that \\(X_i\\) is not in the span of all the \\(\\{ X_j \\}\\) with \\(X_i\\) removed.\n\n\nExercise 3.12 Is the set \\(\\{(1,0),(0,1)\\} \\subset \\mathbb R^2\\) linearly independent? Explain why.\n\n\nExercise 3.13 Is the set \\(\\{(2,1),(2,-1)\\} \\subset \\mathbb R^2\\) linearly independent? Explain why.\n\n\nExercise 3.14 Is the set \\(\\{(1,2),(3,4),(-1,4)\\} \\subset \\mathbb R^2\\) linearly independent? Explain why.\n\nAlthough we chose to define linear independence using Definition 3.4, there are equivalent statements that we could have used. We summarize them in the following theorem.\n\nTheorem 3.3 Let \\(V\\) be a vector space and \\(\\{ X_1 , \\ldots , X_n \\} \\subset V\\). The following statements are equivalent:\n\n\\(\\{ X_1 , \\ldots , X_n \\}\\) is linearly independent in the sense of Definition 3.4.\n\\(0 = a_1 X_1 + \\cdots + a_n X_n\\) has only one solution (when \\(a_i = 0 \\, \\forall i\\)).\nFor every \\(v \\in \\Span  \\{ X_1 , \\ldots , X_n \\}\\), the equation \\(v = a_1 X_1 + \\cdots + a_n X_n\\) has exactly one solution.\n\n\nWhen we say “the following statements are equivalent”, we mean that among (1.), (2.), and (3.) above, there are proofs\n\\[\n(1.) \\, \\Leftrightarrow \\, (2.) \\, \\Leftrightarrow \\, (3.).\n\\]\nSo, we chose to use Definition 3.4 as the definition, and (in the proof of Theorem 3.3) would prove that (2.) is equivalent to Definition 3.4. But we could have just as well used (2.) as the definition, and proved the statement in Definition 3.4 as a theorem. If you look at different books on this subject, you will see different approaches; all of them are equally correct.\nWe omit the proof of Theorem 3.3; the equivalences are not difficult to prove.\nFrom this, we get the following lemma quite easily:\n\nLemma 3.2 Let \\(V\\) be a vector space and \\(S=\\{ X_1 , \\ldots , X_n \\} \\subset V\\).\n\nIf \\(S\\) contains the zero vector, it is linearly dependent.\nIf \\(S\\) contains the same vector twice, it is linearly dependent.\n\n\nNote that this lemma is not an iff statement. For example, not every linearly dependent set contains the zero vector. But if a set does contain the zero vector, this lemma tells you that that set is linearly dependent.\n\nNecessity and sufficiency\nWe are finally in a position to answer the question that began this chapter:\n\nWhat is the minimal amount of information needed to unambiguously describe a linear function?\n\nNote that there are two parts of this statement: how much information suffices to define a linear function, and how much information minimally suffices. These criteria are different.\nFor example, in Chapter 1 we saw that the formula\n\\[\nC(r) := 2 \\pi r\n\\]\nis enough to know \\(C\\) on all its inputs (after all, this formula tells you how to compute \\(C\\) for any input you could pick). So this formula suffices to define the linear function \\(C\\).\nHowever, we saw later in that chapter that, once we know \\(C\\) is linear, this is more information than we need. It is enough to know far less information. For example, suppose I tell you that, in some other universe there are creatures called pumans who have a shape they call a pircle whose pircumference (as they call it) is a function of some measurement they refer to as the pradius of the pircle. Both pradius and pircumference are nonnegative real numbers, thankfully.\nI don’t know the formula for pircumference as a function of pradius (communication with the pumans is very slow and difficult), but I have learned from them two things:\n\npircumference as a function of pradius is linear\nthe pircumference of a pircle of pradius 2025 is 45.\n\nIt turns out that this is enough information to discover the formula for pircumference! Let \\(k \\geq 0\\) be any arbitrary pradius, and call the pircumference function \\(P\\). We can compute\n\\[\nP(k) = P\\left(\\frac{k}{2025} 2025 \\right) = \\frac{k}{2025}P\\left( 2025 \\right)= \\frac{k}{2025}45 = \\frac{k}{45}.\n\\]\nVerify for yourself that you understand why each equality in this computation is true. For the second equality, we used our knowledge that pircumference is linear and for the third equality we used our knowledge that \\(P(2025) = 45\\). If we had been missing either of those facts, we could not conclusively determine the formula for \\(P\\). That means that, once we know \\(P\\) is linear, the knowledge of \\(P\\) on at least one nonzero input is necessary.\nThe following theorem gives two criteria addressing our motivating question at the start of this section: the first criteron describes when we have sufficient information to determine a linear function, the second describes when we have only the necessary (minimal) amount of information.\n\nTheorem 3.4 Let \\(L:A \\to B\\) be linear. Suppose we know the value of \\(L\\) on a set of vectors \\(\\{ V_1 , \\ldots , V_n \\}\\).\n\nIf \\(\\Span \\{ V_1 ,  \\ldots V_n \\} = A\\), we can compute the value of \\(L\\) on all inputs.\nIf \\(\\{ V_1 , \\ldots , V_n \\}\\) is linearly independent, then \\(\\{ V_1 , \\ldots , V_n \\}\\) is a minimal set of information defining \\(L\\) on \\(\\Span \\{ V_1 ,  \\ldots V_n \\}\\).\n\n\nNote that, if \\(\\Span \\{ V_1 ,  \\ldots V_n \\} = A\\) and \\(\\{ V_1 , \\ldots , V_n \\}\\) is linearly independent, this theorem implies that \\(\\{ V_1 , \\ldots , V_n \\}\\) is a minimal choice of information defining \\(L\\) everywhere on \\(A\\).\nThis theorem is very powerful. For example, consider a linear function \\(F : \\mathbb R^3 \\to \\mathbb R^{2025}\\). Theorem 3.4 tells us that if we know only three facts, e.g. the values of \\(L(1,0,0)\\), \\(L(0,1,0)\\), and \\(L(0,0,1)\\), then we are able to use this information to find the value of \\(L\\) on any of the infinitely many vectors in \\(\\mathbb R^3\\). Of course, we would first have to show that \\(\\{(1,0,0),(0,1,0),(0,0,1)\\}\\) spans \\(\\mathbb R^3\\).\nThe situation of having a set of vectors that both spans an entire vector space and is linearly independent is so common, we give it the following name:\n\n\n\nA basis for a vector space\n\nDefinition 3.5 Let \\(V\\) be a vector space and let \\(B\\) be a finite, ordered list of vectors in \\(V\\). If\n\n\\(\\Span B = V\\) and\n\\(B\\) is linearly independent, we say that \\(B\\) is a basis for \\(V\\).\n\n\nWe write list here because order and repetition matter, and these are not qualities of sets.\n\n\n\n\n\n\nNotation\n\n\n\nWe write lists like \\([ 1 , 2, 2 , 3]\\) with square brackets rather than curly ones.2 Note that for lists, \\([1,1,1] \\ne [1]\\) while for sets \\(\\{1,1,1\\} = \\{1\\}\\), and also, for lists \\([1 , 2] \\ne [2,1]\\) while for sets \\(\\{1,2\\} = \\{2,1\\}\\).\n\n\n\nExercise 3.15 Is the list \\([(1,0),(0,1)] \\subset \\mathbb R^2\\) a basis? Explain why.\n\n\nExercise 3.16 Is the set \\([(2,1),(2,-1)] \\subset \\mathbb R^2\\) a basis? Explain why.\n\n\n\n\n\n\n\nNote\n\n\n\nThe preceding exercises demonstrate a very important point: if \\(V\\) has a basis, it is not unique. There are actually many bases for a vector space, and which one you should choose to use depends on the problem at hand.\nFor example, both \\([(1,0), (0,1)]\\) and \\([(1,1), (1,-1)]\\) are bases for \\(\\mathbb R^2\\). If we are dealing with a linear function \\(L : \\mathbb R^2 \\to \\mathbb R\\) where we know \\(L (1,1) = 5\\) and \\(L(1,-1) = 3\\), then it is not useful to work with the basis \\([(1,0), (0,1)]\\).\n\n\nWe now have a restatement of Theorem 3.4 that uses this definition:\n\nTheorem 3.5 Let \\(L:A \\to B\\) be linear. Suppose we know the value of \\(L\\) on a set of vectors \\(\\{ V_1 , \\ldots , V_n \\}\\). If \\([ V_1 , \\ldots , V_n ]\\) is a basis for \\(A\\), then we know the value of \\(L\\) on all its inputs and \\(\\{ V_1 , \\ldots , V_n \\}\\) is a minimal choice of data defining \\(L\\).\n\nWe finally answered the two mysteries that have been motivating us for a few chapters, so we’re done, right?",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases</span>"
    ]
  },
  {
    "objectID": "bases.html#footnotes",
    "href": "bases.html#footnotes",
    "title": "3  Bases",
    "section": "",
    "text": "This is a German word that, in math, means a guess with some free parameters (in this case, \\(a\\) and \\(b\\)) in it. We compute with this guess, and then later find values of the parameters that make the computation valid and also solve our problem. If we cannot find such values, the we have essentially proved that there is no solution to our problem with the form given in the ansatz (although there may be a solution of another form if our ansatz is not general enough).↩︎\nThis is the notation convention used by many modern programming languages.↩︎",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases</span>"
    ]
  },
  {
    "objectID": "vector-spaces-isomorphic-to-euclidean-space.html",
    "href": "vector-spaces-isomorphic-to-euclidean-space.html",
    "title": "4  Vector spaces isomorphic to Euclidean space",
    "section": "",
    "text": "Dimension\nNo, there are still more mysteries!\nOne example is the difference between \\(P_1\\) and \\(\\mathbb R^2\\). We have the following ordered bases for these\n\\[\n[x,1] \\text{ for } P_1 \\text{ and } [(1,0),(0,1)] \\text{ for } \\mathbb R^2.\n\\]\nWhen we have fixed a basis for a vector space, Definition 3.5 together with Theorem 3.3 and Definition 3.3 tell us that, for each vector in our vector space, there is a exactly one way to write that vector as a linear combination of the basis vectors. So, for example, the vector \\(x - 3 \\in P_1\\) can be thought of as \\((1)\\underbrace{(x)}_{\\text{1st basis vector}} + (-3)\\underbrace{(1)}_{\\text{2nd basis vector}}\\).\nIf you and I are thinking of the same basis \\([x,1]\\) for \\(P_1\\), then there is no confusion if we just list the coefficients of this vector relative to this basis. That is, we use the notation \\((1,-3)\\) to refer to the vector \\(x - 3 \\in P_1\\).\nBut \\((1,-3)\\) is a list of two real numbers, so isn’t this an element in \\(\\mathbb R^2\\)? Is \\(P_1\\) the same as \\(\\mathbb R^2\\)?\nOne remarkable fact about \\(P_1\\) is that, although \\([x,1]\\) isn’t the only basis for it (the list \\([x+1, x-1]\\), we have seen, is another), all the bases of \\(P_1\\) have something in common: they have exactly two elements. This can be generalized to the following lemma:\nThis immediately tells us that all bases for \\(P_1\\) have two elements, as well as all bases for \\(\\mathbb R^2\\).\nSince all bases for a vector space are the same length we can refer to this length as a property of the vector space, rather than a property of a particular basis.\nSo, using this notation, \\(\\dim P_1 = 2 = \\dim \\mathbb R^2\\).\nThe following lemma says that if one vector space is inside another, then the dimension of the one on the inside can’t be larger than the one on the outside:",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector spaces isomorphic to Euclidean space</span>"
    ]
  },
  {
    "objectID": "vector-spaces-isomorphic-to-euclidean-space.html#dimension",
    "href": "vector-spaces-isomorphic-to-euclidean-space.html#dimension",
    "title": "4  Vector spaces isomorphic to Euclidean space",
    "section": "",
    "text": "Lemma 4.1 Let \\(V\\) be a vector space with at least one finite basis and let \\(B\\) be that basis. Then all bases of \\(V\\) have the same number of elements as \\(B\\).1\n\n\n\n\nDefinition 4.1 Let \\(V\\) be a vector space with a finite basis of size \\(n\\). Then we say that the dimension of \\(V\\) is \\(n\\) and we write this like \\[\n\\dim V = n.\n\\]\n\n\n\nExercise 4.1 What is the dimension of \\(P_2\\)? What about \\(P_3\\)? What about \\(P_n\\)?\n\n\nExercise 4.2 What is the dimension of the vector space of arrows drawn on a piece of paper? What is the dimension of the vector space of arrows created in a space like a room? (You can think of bits of string of different lengths, with arrows on one end of them, held in space by two people. Addition and multiplication is as for arrows on paper.)\n\n\nExercise 4.3 What is the dimension of \\(\\mathbb C\\) as a vector space over itself?\n\n\nExercise 4.4 What is the dimension of \\(\\mathbb C\\) as a vector space over \\(\\mathbb R\\)?\n\n\n\nLemma 4.2 Let \\(W \\subset V\\) be a vector subspace of the vector space \\(V\\). Suppose \\(W\\) had finite dimension. Then \\[\n\\dim W \\leq \\dim V.\n\\]\n\n\nExercise 4.5 The dimension of the space of continuous functions \\(\\mathbb R \\to \\mathbb R\\) is not finite. Use the result of Exercise 4.1 to explain why.\n\n\nExercise 4.6  \n\nRecall that we know that the function \\(D : C^1 \\to C^0\\) “take the derivative” is linear. Let \\(K\\) be the set of all vectors in \\(C^1\\) which are taken to the 0 vector in \\(C^0\\) by \\(D\\). That is, \\(K\\) is the set of all differentiable functions whose first derivative is 0 everywhere. As an equation, \\[\nK := \\left\\{ f : \\mathbb R \\to \\mathbb R \\left| D(f) = f'(x) = 0  \\right.\\right\\}.\n\\] \\(K\\) is a vector subspace of \\(C^1\\) (you might want to convince yourself that this is true using Theorem 2.1, but you don’t have to for this exercise). What is \\(\\dim K\\)?",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector spaces isomorphic to Euclidean space</span>"
    ]
  },
  {
    "objectID": "vector-spaces-isomorphic-to-euclidean-space.html#the-notion-of-isomorphism",
    "href": "vector-spaces-isomorphic-to-euclidean-space.html#the-notion-of-isomorphism",
    "title": "4  Vector spaces isomorphic to Euclidean space",
    "section": "The notion of isomorphism",
    "text": "The notion of isomorphism\nWe have identified one property that \\(P_1\\) and \\(\\mathbb R^2\\) have in common: they have the same dimension. There is a more general notion of “sameness” that is true for these two vector spaces. We will introduce the word for it here, before explaining precisely what it means in the following section.\nFor the purposes of this book, \\(V\\) and \\(W\\) are isomorphic if they are “the same” once we fix some piece of data.\nIn mathematics more generally, if two objects are isomorphic, it means that they “behave identically” but might need an extra piece of information to actually identify one with the other. The full notion is extremely broad.2",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector spaces isomorphic to Euclidean space</span>"
    ]
  },
  {
    "objectID": "vector-spaces-isomorphic-to-euclidean-space.html#isomorphism-to-mathbb-rn-given-a-basis",
    "href": "vector-spaces-isomorphic-to-euclidean-space.html#isomorphism-to-mathbb-rn-given-a-basis",
    "title": "4  Vector spaces isomorphic to Euclidean space",
    "section": "Isomorphism to \\(\\mathbb R^n\\) (given a basis)",
    "text": "Isomorphism to \\(\\mathbb R^n\\) (given a basis)\nRecall Theorem 2.2, that \\(\\mathbb R^n\\) is the set of lists of length \\(n\\), where the elements of the list are real numbers. Recall that \\(\\mathbb R^n\\) is a vector space, and notice that we can demonstrate at least one basis for \\(\\mathbb R^n\\):\n\\[\n\\begin{aligned}\n&(1,0,0, \\cdots ,0,0) ,\\\\\n&(0,1,0, \\cdots ,0,0), \\\\\n&\\quad \\vdots  \\\\\n&(0,0, \\cdots ,0,1,0), \\\\\n&(0,0,0, \\cdots ,0,1)\n\\end{aligned}\n\\]\nNotice that this basis has exactly \\(n\\) elements.\nNow, suppose we have another vector space \\(V\\) over \\(\\mathbb R\\) with a basis of \\(n\\) elements \\([b_1 , \\ldots , b_n]\\). Given a vector \\(v \\in V\\), there is a unique list of \\(n\\) real numbers \\(v_1 , \\ldots , v_n\\) such that \\[\nv = v_1 b_1 + v_2 b_2 + \\cdots + v_n b_n.\n\\]\nAs an example, consider \\(P_1\\) with basis \\([x+1,x-1]\\). The vector \\(12 \\pi x - 2025\\) can only be written in one way relative to this basis: \\[\n12 \\pi x - 2025 = \\left ( \\frac{12 \\pi - 2025}{2} \\right )(x+1) +  \\left ( \\frac{12 \\pi + 2025}{2} \\right )(x-1) .\n\\]\nOnce we choose a basis, every vector can be “identified” with its list of cooeficients relative to that basis. In the example above, we have \\[\n12 \\pi x - 2025 \\quad ''='' \\quad \\left ( \\frac{12 \\pi - 2025}{2} , \\frac{12 \\pi + 2025}{2} \\right ).\n\\] We put the equals sign in quotes here because these two vectors are not identical (they’re not even in the same vector space), but once we choose a basis, we can think of the left as the right.\n\nExercise 4.7  \n\nChoose a different basis for \\(P_1\\) and write the coordinates of the vector \\(12 \\pi x - 2025\\) relative to that basis.\n\n\n\nExercise 4.8  \n\nChoose any basis you want for \\(P_2\\).\nThe set \\(W:=\\Span\\{ (1 , 0 , 0 , 0) , (0 , 0 , 1 , 0), (0 , 0 , 0 , 1), (1 , 0 , 1 , 0)\\} \\subset \\mathbb R^4\\) is a vector subspace (But you shouldn’t believe me! You can check for yourself!). Choose a basis for this subspace.\nUsing your choses bases, what vector is \\((1,2,3) \\in \\mathbb R^3\\) when interpreted as coordinates for a vector in \\(P_3\\)? How about when interpreted as coordinates for a vector in \\(W\\)?\n\n\nTo make this notion precise, we notice that there is a function (which depends on the basis we choose for \\(P_1\\)) which we’ll name \\(C : P_1 \\to \\mathbb R^2\\). It turns out that this function is a linear function between vector spaces.\nIn particular, adding in \\(P_1\\) and converting to coordinates, adding, and converting back to a polynomial produce the same value:\n\\[\n( a x + b ) + (c x + d) = (a + c) x + (b + d)\n\\] and \\[\n\\begin{aligned}\n\\underbrace{\\left ( \\frac{a +b}{2} , \\frac{a-b}{2} \\right )}_{C(ax + b)} + \\underbrace{\\left ( \\frac{c +d}{2} , \\frac{c-d}{2} \\right )}_{C(cx + d)}\n=& \\left ( \\frac{a +b + c +d}{2} , \\frac{a + c - (b+d) }{2} \\right ) \\\\\nC^{-1} \\left ( \\frac{a +b + c +d}{2} , \\frac{a + c - (b+d) }{2} \\right )\n=&  \\frac{a +b + c +d}{2}(x+1) +  \\frac{a + c - (b+d) }{2} (x-1) \\\\\n=&  (a + c) x + (b + d)\n\\end{aligned}\n\\] where \\(C^{-1}\\) is the inverse function.\nA similar property holds for scalar multiplication.\nIt is perhaps clear that these properties are not unique to \\(P_1\\) and \\(\\mathbb R^2\\), nor to the basis we chose for \\(P_1\\) in the above example. Generalizing this example, we arrive at the following theorem.\n\nTheorem 4.1 Let \\(V\\) be a finite dimensional vector space over the field \\(F\\). Let \\(n = \\dim V\\), and let \\(B\\) be a choice of basis for \\(V\\). Then the function \\(V \\to F^n\\) taking a vector to its list of coordinates relative to \\(B\\) is a linear function which is injective and surjective. (That is, the function is an isomorphism of vector spaces.)\n\n\nExercise 4.9  \n\nConsider two bases \\(B_1 := [x^2 , x , 1]\\) and \\(B_2 := [x^2 , x , -1]\\) for \\(P_2\\).\nConsider the vector \\((17, 5 , 2) \\in \\mathbb R^3\\). What vector in \\(P_2\\) does this represent relative to \\(B_1\\)? What about relative to \\(B_2\\)?\n\n\nWhat this theorem says is that, once we choose a basis for \\(P_1\\), we can treat it the same as \\(\\mathbb R^2\\). We can work with polynomials or coordinates in \\(\\mathbb R^2\\) and we will get the same results. The same thing is true for all vector spaces over \\(\\mathbb R\\) with dimension 2. This fact is generalized in the following corollary:\n\nCorollary 4.1 Let \\(V\\) and \\(W\\) be two vector spaces over the same field of scalars \\(F\\). If \\(\\dim V\\) and \\(\\dim W\\) are finite and equal, then \\(V\\) and \\(W\\) are “the same” (isomorphic) once we choose bases for each of them.\n\nThe proof of this fact is that, since they have the same dimension (call it \\(n\\)), then both \\(V\\) and \\(W\\) can be thought of as lists of coordinates in \\(F^n\\).",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector spaces isomorphic to Euclidean space</span>"
    ]
  },
  {
    "objectID": "vector-spaces-isomorphic-to-euclidean-space.html#basis-dependence",
    "href": "vector-spaces-isomorphic-to-euclidean-space.html#basis-dependence",
    "title": "4  Vector spaces isomorphic to Euclidean space",
    "section": "Basis dependence",
    "text": "Basis dependence\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector spaces isomorphic to Euclidean space</span>"
    ]
  },
  {
    "objectID": "vector-spaces-isomorphic-to-euclidean-space.html#footnotes",
    "href": "vector-spaces-isomorphic-to-euclidean-space.html#footnotes",
    "title": "4  Vector spaces isomorphic to Euclidean space",
    "section": "",
    "text": "For readers familiar with the notion of cardinality, this lemma can be generalized to infinite bases where it states that any two bases have the same cardinality.↩︎\nhttps://en.wikipedia.org/wiki/Isomorphism↩︎",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vector spaces isomorphic to Euclidean space</span>"
    ]
  },
  {
    "objectID": "ways-of-writing-vectors.html",
    "href": "ways-of-writing-vectors.html",
    "title": "5  Ways of writing vectors (relative to a basis)",
    "section": "",
    "text": "Columns\n[TODO]\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ways of writing vectors (relative to a basis)</span>"
    ]
  },
  {
    "objectID": "ways-of-writing-vectors.html#linear-combinations",
    "href": "ways-of-writing-vectors.html#linear-combinations",
    "title": "5  Ways of writing vectors (relative to a basis)",
    "section": "Linear combinations",
    "text": "Linear combinations\n[TODO]\n\nSummation notation\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ways of writing vectors (relative to a basis)</span>"
    ]
  },
  {
    "objectID": "ways-of-writing-vectors.html#other-ways-you-might-encounter",
    "href": "ways-of-writing-vectors.html#other-ways-you-might-encounter",
    "title": "5  Ways of writing vectors (relative to a basis)",
    "section": "Other ways you might encounter",
    "text": "Other ways you might encounter\n[TODO]\n\nRows\n[TODO]\n\n\nEinstein summation notation\n[TODO]\n\n\nbra-ket\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ways of writing vectors (relative to a basis)</span>"
    ]
  },
  {
    "objectID": "ways-of-writing-linear-maps.html",
    "href": "ways-of-writing-linear-maps.html",
    "title": "6  Ways of writing linear maps",
    "section": "",
    "text": "Maps from 1-dimensional spaces\n[TODO]\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ways of writing linear maps</span>"
    ]
  },
  {
    "objectID": "ways-of-writing-linear-maps.html#maps-to-1-dimensional-spaces",
    "href": "ways-of-writing-linear-maps.html#maps-to-1-dimensional-spaces",
    "title": "6  Ways of writing linear maps",
    "section": "Maps to 1-dimensional spaces",
    "text": "Maps to 1-dimensional spaces\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ways of writing linear maps</span>"
    ]
  },
  {
    "objectID": "ways-of-writing-linear-maps.html#maps-from-2-dimensions-to-2-dimensions",
    "href": "ways-of-writing-linear-maps.html#maps-from-2-dimensions-to-2-dimensions",
    "title": "6  Ways of writing linear maps",
    "section": "Maps from 2-dimensions to 2-dimensions",
    "text": "Maps from 2-dimensions to 2-dimensions\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ways of writing linear maps</span>"
    ]
  },
  {
    "objectID": "ways-of-writing-linear-maps.html#maps-in-finite-dimensions",
    "href": "ways-of-writing-linear-maps.html#maps-in-finite-dimensions",
    "title": "6  Ways of writing linear maps",
    "section": "Maps in finite dimensions",
    "text": "Maps in finite dimensions\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ways of writing linear maps</span>"
    ]
  },
  {
    "objectID": "ways-of-writing-linear-maps.html#maps-involving-infinite-dimensional-spaces",
    "href": "ways-of-writing-linear-maps.html#maps-involving-infinite-dimensional-spaces",
    "title": "6  Ways of writing linear maps",
    "section": "Maps involving infinite dimensional spaces",
    "text": "Maps involving infinite dimensional spaces\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ways of writing linear maps</span>"
    ]
  },
  {
    "objectID": "ways-of-writing-linear-maps.html#composing-maps",
    "href": "ways-of-writing-linear-maps.html#composing-maps",
    "title": "6  Ways of writing linear maps",
    "section": "Composing maps",
    "text": "Composing maps\n[TODO]\n\nNoncommutativity\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ways of writing linear maps</span>"
    ]
  },
  {
    "objectID": "ways-of-writing-linear-maps.html#adding-maps",
    "href": "ways-of-writing-linear-maps.html#adding-maps",
    "title": "6  Ways of writing linear maps",
    "section": "Adding maps",
    "text": "Adding maps\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ways of writing linear maps</span>"
    ]
  },
  {
    "objectID": "ways-of-writing-linear-maps.html#matrices-are-sort-of-like-numbers",
    "href": "ways-of-writing-linear-maps.html#matrices-are-sort-of-like-numbers",
    "title": "6  Ways of writing linear maps",
    "section": "Matrices are sort of like numbers",
    "text": "Matrices are sort of like numbers\n[TODO]\n\nNumbers are \\(1 \\times 1\\) matrices\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ways of writing linear maps</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Axler, Sheldon. 2024. Linear Algebra Done Right. Undergraduate\nTexts in Mathematics. Springer, Cham. https://doi.org/10.1007/978-3-031-41026-0.\n\n\nDummit, David S., and Richard M. Foote. 2004. Abstract Algebra.\nThird Ed. John Wiley & Sons, Inc., Hoboken, NJ.\n\n\nHefferon, Jim. 2022. Linear Algebra. Fourth Ed. https://hefferon.net/linearalgebra/.\n\n\nTreil, Sergei. 2024. Linear Algebra Done Wrong. https://sites.google.com/a/brown.edu/sergei-treil-homepage/linear-algebra-done-wrong.",
    "crumbs": [
      "References"
    ]
  }
]