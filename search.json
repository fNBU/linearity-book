[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linearity",
    "section": "",
    "text": "What are these notes?\nThese are notes for a first course in Linear Algebra.\n\nThe PDF version of these notes can be fount at https://fnbu.pw/linearity-book/Linearity.pdf.",
    "crumbs": [
      "What are these notes?"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Why publish a new set of Linear Algebra notes?\nThis section discusses why these notes exist. Students may skip this section.\nLinear Algebra, like Calculus, is one of the math subjects with the most textbooks, so it’s reasonable to ask why a new set of notes is needed. Plainly, I looked at the six open-access books on the subject on the AIMath website and found that none of them were fit for my purpose (detailed below).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#the-perspective-of-these-notes",
    "href": "preface.html#the-perspective-of-these-notes",
    "title": "Preface",
    "section": "The perspective of these notes",
    "text": "The perspective of these notes\nThese notes are constructed to vindicate the following objectives:\n\nIt is morally right for course materials to be free. Few existing books on this subject in English satisfy this criterion (eg Hefferon (2022) is GFDL or CC BY-SA 3.0 US, Treil (2024) is CC BY-NC-ND 3.0 with source unavailable). This book, and the source used to generate it are freely available with a permissive license.\nIn practice, scientists, engineers, etc. need to be able to recognize linearity so that they may choose the correct solution techniques. They also need to understand why linear problems are preferable to non-linear ones so that they might try to massage their current problem into a linear one.\nWhen we say “solution techniques” as above, 99% of the time we mean software packages. Mathematicians and physicists teach linear algebra techniques in colleges and universities, and emphasize by-hand solution techniques for historical and cultural reasons. Most working people who encounter such problems do not use such techniques, they recognize that their problem is linear and offload the problem to a software package. Mathematicians and physicists generally get a second pass at learning linear algebra in a more theory-heavy context (at the very least when learning modules), and so do not need that approach in a first course.\nThe usefulness of linear algebra techniques stems wholly from the homomorphism property of linear maps:\n\\[ L(a V + b W) = a L(V) + b L(W) \\]\nNo introductory, open-access, English language books on the topic that I am aware of motivate the study of the subject with this point. They traditionally begin with coordinate geometry or solving systems of linear equations. It is a very mathematicians’ way of thinking to motivate study of a topic by identifying a class of equations and asking “How do we solve them? What properties do they have?” This is not a way of thinking that is useful for people encountering linearity in the wild.\nMany advanced undergraduate books like Axler (2024) begin with the definition of a vector space.\nIt is also a very mathematician’s way of thinking to begin with a definition of a set of objects which will be mapped into or out of (viz. most treatments of Abstract Algebra). To talk about maps (which are really the objects of interest), surely we must first talk about (co)domains!\nCoordinate geometry is, at least, a class of real problems where linear techniques naturally arise, but the relevance of this as an example from “the wild” has basically vanished in the last 70 years. Today’s scientists and engineers are more likely to encounter linearity in optimization, data science, machine learning, or numerical PDEs.\n\nSo, for these reasons, I set out to write my own course notes.\n\n\n\n\nAxler, Sheldon. 2024. Linear Algebra Done Right. Undergraduate Texts in Mathematics. Springer, Cham. https://doi.org/10.1007/978-3-031-41026-0.\n\n\nHefferon, Jim. 2022. Linear Algebra. Fourth Ed. https://hefferon.net/linearalgebra/.\n\n\nTreil, Sergei. 2024. Linear Algebra Done Wrong. https://sites.google.com/a/brown.edu/sergei-treil-homepage/linear-algebra-done-wrong.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "what-is-linearity.html",
    "href": "what-is-linearity.html",
    "title": "1  What is Linearity?",
    "section": "",
    "text": "Two properties\nThe function \\(C : \\mathbb R \\to \\mathbb R\\) given by\n\\[ C(r) := 2 \\pi r \\]\ncomputes the circumference of a circle, given its radius.\nThe circumference of a circle has a couple nice properties. First, the circumference of a circle of radius 14 is twice the circumference of a circle of radius 7:\n\\[\n\\begin{aligned}\nC(7) &= 2 \\pi (7) = 14 \\pi   \\\\\nC(14) &= 2 \\pi (14) = 28 \\pi   \n\\end{aligned}\n\\]\nThis holds in general; multiplying the radius of a circle by \\(k\\) also changes the circumference by a factor of \\(k\\):\n\\[ C(k r) = k C(r) . \\]\nFurthermore, adding any amount to the radius increases the circumference in a predictable way:\n\\[ C(r_1 + r_2) = C(r_1) + C(r_2). \\]\nIt’s a bit remarkable that these two properties hold not just for circles; scaling any shape in the plane (with circumference \\(c\\)) by a factor of \\(k\\) multiplies its circumference by \\(k\\) (its new circumference is \\(kc\\)), and increasing the scale by a constant \\(s\\) increases its circumference by \\(sc\\).\nWhat other functions have the following two properties?\n\\[\n\\begin{aligned}\n\\text{Property 1:} \\quad & f(a x )= a f(x) \\\\\n\\text{Property 2:} \\quad & f( x + y ) = f(x) + f(y)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Linearity?</span>"
    ]
  },
  {
    "objectID": "what-is-linearity.html#two-properties",
    "href": "what-is-linearity.html#two-properties",
    "title": "1  What is Linearity?",
    "section": "",
    "text": "Exercise 1.1 Demonstrate that the area of a circle as a function of its radius does not satisfy properties 1 and 2.\n\n\nExercise 1.2 Can you think of any shape in the plane whose area (as a function of scaling) satisfies properties 1 and 2? If yes, which? If no, why not?\n\n\nExercise 1.3 Can you think of any shape in the 3-dimensional space whose volume (as a function of scaling) satisfies properties 1 and 2? If yes, which? If no, why not?\n\n\nClassification of functions satisfying Properties 1 and 2\nSo far, we have seen that some functions satisfy properties 1 and 2, and others do not.\n\nFunctions that do and don’t satisfy properties 1 and 2\n\n\n\n\n\n\ndo\ndon’t\n\n\n\n\ncircumference of a circle as a function of radius\narea of a circle as a function of radius\n\n\ncircumference of any shape in the plane as a function of scale\narea of any shape in the plane as a function of scale\n\n\n\nvolume of any shape in 3-dimensions as a function of scale\n\n\n\n\nWhy does scaling satisfy properties 1 & 2 for any shape, not just circles?\nWe asserted above that property 1 is not just satisfied by circles (when you scale the radius) but is satisfied by all curves. Why is this the case?\nWhat do we mean by scaling a figure in the plane by a factor of 2? Well, a reasonable answer is to say that a figure is a set of points and each point has an \\(x\\) and \\(y\\) coordinate. For example, the circle of radius \\(r\\) is the set of points with coordinates given by\n\\[\n\\begin{aligned}\nx =& r \\cos \\theta \\\\\ny =& r \\sin \\theta\n\\end{aligned}\n\\]\nwhere \\(\\theta \\in [0,2\\pi]\\).\n\n\n\n\n\n\nNotation\n\n\n\nWe introduced a symbol above: \\(\\in\\). This literally means “in” or “is in” depending on context.\nFor example, \\(\\pi \\in \\mathbb R\\) means “pi is in the real numbers”.\nSometimes we will want to list the set first, like \\(\\mathbb R \\ni \\pi\\). In this case we read it as “the real numbers contain pi.”\n\n\n\n\n\n\n\n\nNotation\n\n\n\nWhen we want to refer to a set, we will often use notation like the following:\n\\[\n\\left\\{ (x,y) \\in \\mathbb R^2 \\left| \\begin{aligned} x =& r \\cos \\theta \\\\ y =& r \\sin \\theta \\end{aligned} \\quad, \\theta \\in [0,2\\pi] \\right. \\right\\}.\n\\]\nIn general, this notation has the form\n\\[\nB=\\left\\{ f(x_1,x_2,...) \\in A \\left| \\begin{aligned} & \\text{ constraints involving }  \\\\ & x_1,x_2,... \\end{aligned} \\right. \\right\\}.\n\\]\nThe process for constructing the set \\(B\\) is the following:\n\nFind all the \\(x_i\\) that satisfy the constraints to the right of the \\(\\mid\\) symbol.\nPlug all the \\(x_i\\) you found in the previous step into the function \\(f\\).\nThe function \\(f\\) produces things in \\(A\\), and so the set of all the things you produced in the last step is a collection of some (but not necessarily all) of the things in \\(A\\).\nThis is the set \\(B\\).\n\n\n\nConsider the point \\((4,2)\\) in the plane. We can think of this point as the “sum” of its \\(x\\) and \\(y\\) coordinates:\n\\[\n(4,2) = (4,0) + (0,2).\n\\]\nTo scale this point by a factor of 2, it seems reasonable to multiply both coordinates by 2:\n\\[\nS_2((4,2)) = (8,4).\n\\]\nNotice that the function \\(S_2 : \\mathbb R^2 \\to \\mathbb R^2\\) “multiply coordinates by 2” has properties 1 and 2.\n\nExercise 1.4 Verify this.\n\n\nExercise 1.5 Check that applying \\(S_2\\) to a circle of radius \\(r\\) produces a circle of radius \\(2r\\).\n\nNow, for any curve \\(\\gamma : [a,b] \\to \\mathbb R^2\\) given by\n\\[\n\\gamma(t) = (x(t) , y(t)),\n\\]\nits length can be computed by\n\\[\n\\int _{a}^{b}{\\sqrt {x'(t)^{2}+y'(t)^{2}}} \\quad dt.\n\\]\n(You may have seen this in multivariable calculus or physics.) Consider the composition\n\\[\n[a,b] \\xrightarrow{\\quad \\gamma \\quad} \\mathbb R^2 \\xrightarrow{\\quad S_2 \\quad} \\mathbb R^2\n\\]\nThe formula is\n\\[\n( S_2 \\circ \\gamma )(x) =  ( 2 x(t) , 2 y(t) )\n\\]\nand the length is given by\n\\[\n\\begin{aligned}\n&\\int _{a}^{b}{\\sqrt {\\left(\\left[2x(t)\\right]'\\right)^{2}+\\left(\\left[2y(t)\\right]'\\right)^{2}}} \\quad dt \\\\\n=&\\int _{a}^{b}{\\sqrt {\\left(2x'(t)\\right)^{2}+\\left(2y'(t)\\right)^{2}}} \\quad dt \\\\\n=&\\int _{a}^{b}{\\sqrt {4\\left(x'(t)\\right)^{2}+4\\left(y'(t)\\right)^{2}}} \\quad dt \\\\\n=&\\int _{a}^{b}{2\\sqrt {\\left(x'(t)\\right)^{2}+\\left(y'(t)\\right)^{2}}} \\quad dt \\\\\n=&2\\int _{a}^{b}{\\sqrt {\\left(x'(t)\\right)^{2}+\\left(y'(t)\\right)^{2}}} \\quad dt\n\\end{aligned}\n\\]\nNotice that what we obtain on the last line is exactly twice the length of the curve \\(\\gamma\\). Convince yourself that there is nothing special about the number 2 here; if we had replaced \\(S_2\\) by \\(S_{17}\\), then we would have obtained 17 times the length of \\(\\gamma\\) in the last line.\n\n\nMore functions which break up over “sums”\n\nDifferentiation\nNow consider how we compute the derivative of a function like the following:\n\\[\n\\begin{aligned}\n\\frac{d}{dx} \\left[ 2x^2 + x \\right] &= \\frac{d}{dx} \\left[ 2x^2 \\right] + \\frac{d}{dx} \\left[ x \\right] \\\\\n&= 2\\frac{d}{dx} \\left[ x^2 \\right] + \\frac{d}{dx} \\left[ x \\right] \\\\\n&= 4x + 1\n\\end{aligned}\n\\]\nIf we let \\(D : \\text{FUNCTIONS} \\to \\text{FUNCTIONS}\\) be the operation of taking a derivative, then in the first line we used\n\\[ D[f_1(x) + f_2(x)] = D[f_1(x)] + D[f_2(x)] \\]\nand in the second line we used the fact that, when \\(k\\) is constant,\n\\[ D[k f(x)] = k D[f(x)]. \\]\nThus, \\(D\\) (that is, differentiation) satisfies properties 1 and 2. (Although you should probably be uncomfortable that we wrote \\(D : \\text{FUNCTIONS} \\to \\text{FUNCTIONS}\\) above. What is the set \\(\\text{FUNCTIONS}\\)? Are all functions differentiable? We will address this later. For now, it suffices to replace \\(\\text{FUNCTIONS}\\) above with \\(P_n\\), the set of all polynomials of degree at most \\(n\\). In fact, it is the case that \\(D : P_n \\to P_{n-1}\\) if \\(n\\geq 1\\).)\n\n\nDefinite integration\nFix an interval \\([a,b]\\) and consider \\[\nI(f) := \\int_a^b f(x) \\quad dx.\n\\]\n\nExercise 1.6 Check that \\(I\\) has properties 1 and 2.\n\n\n\n\nWhy are properties 1 and 2 useful?\nWhy is it useful that \\(D\\) satisfies properties 1 and 2? It allows us to compute derivatives of complicated expressions like \\(2x^2 + x\\) if we only know the computation on some simple parts of the expression. Knowing the derivative of \\(x^2\\) and \\(x\\) is all that is needed.\nSimilarly, if we know the circumference of a shape in the plane at one scale, we can compute its circumference at all scales using property 1.\nNot all functions are linear, but if a function is linear, it is much easier to compute with.",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Linearity?</span>"
    ]
  },
  {
    "objectID": "what-is-linearity.html#linear-functions",
    "href": "what-is-linearity.html#linear-functions",
    "title": "1  What is Linearity?",
    "section": "Linear functions",
    "text": "Linear functions\n\nDefinition 1.1 (linear function) A function satisfying properties 1 and 2 is called linear.\n\n\n\n\n\n\n\nWarning\n\n\n\nWe use the term “linear” for these functions, but we also use the word “line” for graphs in the plane with formula \\(f(x) = m x + b\\). This term is overloaded and means different things in these two contexts.\n\n\n\nExercise 1.7 Show that \\(f(x) = m x + b\\) is only a linear function when \\(b = 0\\).",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Linearity?</span>"
    ]
  },
  {
    "objectID": "the-notion-of-a-vector-space.html",
    "href": "the-notion-of-a-vector-space.html",
    "title": "2  The notion of a Vector Space",
    "section": "",
    "text": "Definition\nIf \\(L : A \\to B\\) is linear, what must be true about \\(A\\) and \\(B\\)?\nLet’s go back to the definition of a linear function. A function is linear if and only if it satisfies the following two properties:\n\\[\n\\begin{aligned}\n\\text{Property 1:} \\quad & L(a x )= a L(x) \\\\\n\\text{Property 2:} \\quad & L( x + y ) = L(x) + L(y).\n\\end{aligned}\n\\]\nLet’s list a few things that must be true to arrive at these expressions:\nThere are perhaps some more properties that would be nice, and that are true about all the domains and codomains of linear functions we have seen so far:\nEventually, mathematicians (who were working with linear functions intuitively) worked out the minimal set of facts that one needs about the domain and codomain of a linear function for everything to be coherent. Here it is\nNote that the notation \\(+ \\colon V \\times V \\to V\\) implies that \\(V\\) is, in particular, closed under addition (since \\(V\\) is the codomain of \\(+\\); addition cannot by definition return an element not in \\(V\\)). Similarly we can conclude that \\(V\\) must be closed under scalar multiplication.\nAll of the properties above hold (with the field of scalars \\(F\\) set to \\(\\mathbb R\\)) for all the spaces we have used so far as the domain and codomain of a linear function. (You may want to check this for yourself quickly. Most of these facts obviously hold for polynomials, functions, points in \\(\\mathbb R^2\\), and \\(\\mathbb R\\) itself. However, one must still check all of them before using a set as the domain or codomain of a linear function.)\nNot all sets of mathematical objects satisfy these properties, though. An example is the set of points on the sphere. It turns out that there is no way (although the proof will have to wait for a future chapter) to turn that set into a vector space.\nThe way you should think about this definition is the following:\nAn example of the kind of problem you can run into is the following: Let \\(L:A \\to B\\) be linear and suppose \\(A\\) is a vector space as defined above but that \\(B\\) fails to have an additive identity (that is, there is no zero vector in \\(B\\)). We can compute\n\\[\nL(0) = 0L(0) = 0 (?) = 0\n\\] where the \\(?\\) stands for whatever \\(L(0)\\) maps to. Notice that in the end, it doesn’t matter because we conclude that \\(L(0) = 0 \\in B\\). But this is nonsensical since we assumed there was no zero vector in \\(B\\).",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The notion of a Vector Space</span>"
    ]
  },
  {
    "objectID": "the-notion-of-a-vector-space.html#definition",
    "href": "the-notion-of-a-vector-space.html#definition",
    "title": "2  The notion of a Vector Space",
    "section": "",
    "text": "Definition 2.1 A vector space is a set \\(V\\) of vectors and a set \\(F\\) of scalars that satisfy the following properties\n\nthe set of scalars \\(F\\) is a field (see the next section for more information on what we mean by field),\nthere is a function \\(+ \\colon V \\times V \\to V\\), called vector addition that\n\nis associative: \\((X+Y) + Z = X + (Y + Z)\\),\nis commutative: \\(X+Y = Y+X\\),\nhas a \\(0\\) (an additive identity) which means that \\(0\\) satisfies \\(0 + X = X\\) for all \\(X \\in V\\),\nhas negatives (additive inverses) which means that for each \\(X \\in V\\) there is an element \\(Y \\in V\\) such that \\(X + Y = 0\\) (one can prove that there is only one inverse of \\(X\\), and this is usually written \\(-X\\)),\n\nthere is a function \\(\\cdot \\colon F \\times V \\to V\\), called scalar multiplication such that\n\nscalar multiplication associates with field multiplication (that is, \\(\\forall a , b \\in F, X \\in V\\) it’s true that \\((ab)X = a (bX)\\))\nthe multiplicative unit in \\(F\\) is a unit for scalar multiplication (that is, \\(\\forall X \\in V\\) we have \\(1 X = X\\))\n\nthe following distributive laws hold:\n\n\\((a+b) X = aX + bX , \\quad \\forall a, b\\in F , X \\in V\\)\n\\(a(X+Y) = aX + aY , \\quad \\forall a \\in F ,  X,Y \\in V\\)\n\n\n\n\n\n\n\n\n\nNotation\n\n\n\nAbove, we used the symbol \\(\\forall\\). This is a symbol that mathematicians use that literally just means “for all”.\nSo, as an example \\(x \\in P_3 , \\forall x \\in P_2\\) is read as “for all x in the set of second degree polynomials, x is in the set of third degree polynomials”, which is just a wordy way of saying that all second degree polynomials are third degree polynomials.\n\n\n\n\n\n\n\nwe are allowed to use a set \\(A\\) satisfying all of the properties in Definition 2.1 as the domain or codomain of a linear function, but\na set \\(A\\) that fails to satisfy any of the properties above can never be used as the domain or codomain of a linear function. If you tried to use it in this way, you would eventually run into statements that make no sense.1\n\n\n\n\nExercise 2.1 Check that \\(\\mathbb R\\) forms a vector space (the set of scalars is \\(\\mathbb R\\)).\n\n\nExercise 2.2 Check that \\(\\mathbb R^2\\) is a vector space.",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The notion of a Vector Space</span>"
    ]
  },
  {
    "objectID": "the-notion-of-a-vector-space.html#scalars",
    "href": "the-notion-of-a-vector-space.html#scalars",
    "title": "2  The notion of a Vector Space",
    "section": "Scalars",
    "text": "Scalars\nIn the above definition, we said that the scalars for a vector space must come from a field. What does this mean?\nFor the purposes of this text, our field will always be \\(\\mathbb R\\) or the field of complex numbers:\n\\[\n\\mathbb C := \\{ a + b i \\mid a,b \\in \\mathbb R \\}\n\\]\nwhere the symbol \\(i\\) has the property that \\(i^2 = -1\\) (and so we sometimes write it as \\(\\sqrt{-1}\\)). These two sets are the set of scalars for most vector spaces found in applications in the wild.\n\nExercise 2.3 Check that \\(\\mathbb C\\) can be thought of as a vector space with field of scalars \\(\\mathbb R\\).\n\n\nDefinition 2.2 Let \\(P_n\\) be the set (which we mentioned informally in the previous chapter)\n\\[\n\\left\\{ a_{n} x^n + a_{n-1} x^n + \\cdots + a_1 x + a_0 \\mid a_n , a_{n-1}, \\ldots , a_1 , a_0 \\in \\mathbb R \\right\\}.\n\\]\nThis is called the set of polynomials of degree \\(n\\).\n\n\nExercise 2.4 Is \\(P_n\\) a vector space for each \\(n \\in \\mathbb N\\)?\n\nFrom the previous two examples, you might notice that the definitions\n\\[\n\\mathbb C := \\{ a + b i \\mid a,b \\in \\mathbb R \\}\n\\]\nand\n\\[\nP_n := \\left\\{ a_{n} x^n + a_{n-1} x^n + \\cdots + a_1 x + a_0 \\mid a_n , a_{n-1}, \\ldots , a_1 , a_0 \\in \\mathbb R \\right\\}\n\\]\nlook similar, and they give you a hint about how to think of these as vector spaces; in both cases, they are constructed as the sum of things with coefficients in \\(\\mathbb R\\). We will investigate this further in the next chapter.\n\nFields\nIn case you are interested, the full definition of a field can be found on Wikipedia. A field that you already know about, but that is not \\(\\mathbb R\\) or \\(\\mathbb C\\) is the field of rational numbers:\n\\[\n\\mathbb Q := \\left\\{ \\left. \\frac{a}{b}  \\right| a,b \\in \\mathbb Z ,  b \\ne 0  \\right\\}.\n\\]\nThere are esoteric examples of fields, and also algebraic structures stranger than fields. If this is interesting to you, you might try to take a course in Abstract Algebra. I learned this topic from Liz Stanhope and Dummit and Foote (2004).\nIn computing, real numbers (which do form a field) are often represented by 32 or 64 bit floating point numbers. It is perhaps interesting to know that floating point numbers (in any number of bits) (which are encountered often in the wild) do not form a field because addition of floating point numbers is not associative.",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The notion of a Vector Space</span>"
    ]
  },
  {
    "objectID": "the-notion-of-a-vector-space.html#subspaces",
    "href": "the-notion-of-a-vector-space.html#subspaces",
    "title": "2  The notion of a Vector Space",
    "section": "Subspaces",
    "text": "Subspaces\nWe have seen in this chapter that if you want to do something with a linear function, you better check first that its domain and codomain are vector spaces. It’s quite a lengthy process to verify all the properties in Definition 2.1, even if in many cases it is not difficult to verify each one individually. We are in need of machines that allow us to know that something is a vector space more easily. That is what this and the next section are about.\nHow do we know that a set \\(W \\subset V\\) is a vector space?\n\n\n\n\n\n\nNotation\n\n\n\nAbove, we used the symbol \\(\\subset\\). This just means “is a sub set of”, for example \\(\\mathbb Z \\subset \\mathbb Q\\) means “the set of integers is a subset of the set of rational numbers.”\nWhen we say “A is a subset of B” or, equivalently write \\(A \\subset B\\), we mean that everything in \\(A\\) is also in \\(B\\).\nWe will sometimes write \\(A \\supset B\\) to mean that \\(B\\) is a subset of \\(A\\) or that “A has B as a subset.”\n\n\nWe’ve already seen some examples of this. For example, we know that differentiable functions form a vector space, and also the set \\(P_n\\) forms a vector space. Since every polynomial is differentiable, it follows that \\(P_n \\subset \\{ \\text{differentiable functions}\\}\\). So \\(P_n\\) is an example of a subset of a vector space that turns out to be a vector space itself.\nLet’s introduce some terminology for this situation.\n\nDefinition 2.3 A subset \\(W\\) of a vector space \\(V\\) is called a (vector) subspace if \\(W\\) is itself a vector space (with field of scalars \\(F\\), addition, multiplication, zero vector inherited from \\(V\\)).\n\nWe have already seen an example of a subspace above. A nonexample is the set of points in the plane at distance 1 from the origin. This set doesn’t satisfy the closure assumptions of Definition 2.1.\nIt is a bit remarkable (although not difficult to prove) that we actually only need to check the closure assumptions. The following theorem encodes this fact.\n\nTheorem 2.1 A subset \\(W\\) of a vector space \\(V\\) is itself a vector space (with field of scalars \\(F\\), addition, multiplication, zero vector inherited from \\(V\\)) iff the following two properties hold:\n\n\\(W\\) is closed under scalar multiplication: \\(aX \\in W \\quad \\forall a \\in F, X \\in W\\)\n\\(W\\) is closed under vector addition: \\(X+Y \\in W \\quad \\forall X,Y \\in W\\)\n\n\nWe won’t do the proof here (you can go to e.g. Hefferon (2022) to see it), but we will sketch it.\nSince the scalars, and the operations \\(\\cdot , +\\) are inherited from the ambient space \\(V\\), they are known to satisfy associativity, commutativity, existence of inverses and zeros, etc (because we had to prove those things when we showed that \\(V\\) was a vector space). However, in Definition 2.1, we wrote \\(+ \\colon V \\times V \\to V\\). That is, \\(+\\) takes two vectors in \\(V\\) and returns a vector in \\(V\\). Since \\(W \\subset V\\), we know that we can use elements of \\(W\\) as inputs to \\(+\\). That is, we know \\(+ \\colon W \\times W \\to V\\), but we cannot know addition of vectors always lands in the subset \\(W\\). The first assumption above tells us this. Similarly for scalar multiplication.\nFor the proof in the opposite direction, notice that for \\(W\\) to be a vector space we must have \\(+ \\colon W \\times W \\to W\\). If addition is not closed, then \\(+\\) has the wrong codomain, and \\(W\\) isn’t a vector space. Similarly if \\(W\\) is not closed under scalar multiplication.\n\nEasier subspaces\nNotice that we can use Theorem 2.1 to quite easily prove (assuming we know differentiable functions form a vector space) that \\(P_n\\) is a vector space. We can also conclude easily that, for example,\n\\[\n\\{ a \\sin x + b \\cos x \\mid a,b \\in \\mathbb R \\}\n\\]\nis a vector space.\n\nExercise 2.5 Is the set of arrows that are either fully horizontal or fully vertical a vector subspace of the vector space of arrows in 2D? Why?\n\n\nExercise 2.6 Consider the set of monomials\n\\[\n\\{ 1 , x , x^2 , x^3 , \\ldots  \\} .\n\\]\nIf I pick any two of these (say \\(x^k\\) and \\(x^l\\) with \\(k \\ne l\\), eg \\(x^{17}\\) and \\(x^{2025}\\)) and form the set\n\\[\n\\{ a x^k + b x^l \\mid a,b \\in \\mathbb R \\} ,\n\\]\nis this a vector subspace of the space of differentiable functions? Why?",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The notion of a Vector Space</span>"
    ]
  },
  {
    "objectID": "the-notion-of-a-vector-space.html#the-space-of-lists-of-length-n",
    "href": "the-notion-of-a-vector-space.html#the-space-of-lists-of-length-n",
    "title": "2  The notion of a Vector Space",
    "section": "The space of lists of length \\(n\\)",
    "text": "The space of lists of length \\(n\\)\nWe have seen that the following are vector spaces: \\(\\mathbb R\\) over \\(\\mathbb R\\), \\(\\mathbb R^2\\) over \\(\\mathbb R\\), \\(\\mathbb C\\) over \\(\\mathbb C\\).\nThere is a very powerful theorem that we can prove to generate many vector spaces that are of a similar form.\n\nTheorem 2.2 Let \\(F\\) be any field. Define the following set\n\\[\nF^n := \\{ (a_1 , a_2 , \\cdots , a_n) \\mid a_i \\in F \\}.\n\\]\nThis is the space of lists of length \\(n\\), with members from \\(F\\).\nLet \\(+ : F^n \\times F^n \\to F^n\\) be defined by\n\\[\n(a_1 , a_2 , \\cdots , a_n) + (b_1 , b_2 , \\cdots , b_n) = (a_1 + b_1 , a_2 + b_2 , \\cdots , a_n + b_n )\n\\]\nand let \\(\\cdot : F \\times F^n \\to F^n\\) be defined by\n\\[\nx (a_1 , a_2 , \\cdots , a_n) = (xa_1 , xa_2 , \\cdots , xa_n).\n\\]\nThen \\(F^n\\) with these two operations (and field of scalars \\(F\\)) is a vector space.\n\nThe proof is a straightforward check of all the items in Definition 2.1.\nNote that \\(\\mathbb R, \\mathbb R^2 , \\mathbb C\\) can be proved to be vector spaces using this theorem.\nTo develop your intuition for what this looks like, let’s do a multiplication in \\(\\mathbb C^4\\):\n\\[\n\\begin{aligned}\n& i (1 , i , 1 + i , 2 - 3 i)\\\\\n=& (i , i^2 , i + i^2 , 2 i - 3 i^2) \\\\\n=& ( i , -1 , i -1 , 2i +3).\n\\end{aligned}\n\\]\nNote that this theorem only works if \\(F^n\\) is a vector space over \\(F\\). So, above, \\(\\mathbb C^4\\) is a vector space with scalars \\(\\mathbb C\\).\n\nExercise 2.7 Is \\(\\mathbb R^n\\) a vector subspace of \\(\\mathbb C^n\\)? Think very carefully about Definition 2.3.\n\n\n\n\n\nDummit, David S., and Richard M. Foote. 2004. Abstract Algebra. Third Ed. John Wiley & Sons, Inc., Hoboken, NJ.\n\n\nHefferon, Jim. 2022. Linear Algebra. Fourth Ed. https://hefferon.net/linearalgebra/.",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The notion of a Vector Space</span>"
    ]
  },
  {
    "objectID": "the-notion-of-a-vector-space.html#footnotes",
    "href": "the-notion-of-a-vector-space.html#footnotes",
    "title": "2  The notion of a Vector Space",
    "section": "",
    "text": "It is the case that some maps between spaces that are weaker than vector spaces have defining properties 1 and 2. The spaces are a generalization of vector spaces called modules and the maps are called module homomorphisms. If you look at the definition of module homomorphism on wikipedia, you’ll see the same two equations we used to define linear functions.↩︎",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The notion of a Vector Space</span>"
    ]
  },
  {
    "objectID": "bases.html",
    "href": "bases.html",
    "title": "3  Bases",
    "section": "",
    "text": "What is the minimal amount of information needed to unambiguously describe a linear function?\nRemember that the derivative function \\(D(f) := \\frac{df}{dx}\\) allows us to compute the derivative of, for example\n\\[\nf(x) = x^2 + 4 \\sin x\n\\]\nif we know how to compute \\(D\\) on only \\(x^2\\) and \\(\\sin x\\). In fact, knowing just how to compute \\(D\\) on these two functions and knowing that \\(D\\) is linear allows us to compute the derivative of any function of the form\n\\[\nx \\mapsto a x^2 + b \\sin x , \\quad a,b \\in \\mathbb R.\n\\]\nFrom a very small amount of information, we actually know a lot about \\(D\\). Thus, we begin this chapter straightforwardly with a question, which is the title of the next section:\nConsider the following example:\nAssume that \\(L : P_2 \\to P_0\\) is linear and that\n\\[\nL(x^2 + 3x) = 4.\n\\]\nCan we determine \\(L\\) on every element of \\(P_2\\)? That is, can we compute\n\\[\nL(a x^2 + b x + c)?\n\\]\nLet’s try our best. We would like to isolate a term that looks like \\(x^2 + 3x\\) because that is something we have information about.\n\\[\n\\begin{aligned}\nL(a x^2 + b x + c)=& L(a x^2 + b x)  + L(c) \\quad &\\text{by property 1} \\\\\n=& L\\left(a \\left[ x^2 + \\frac{b}{a} x \\right] \\right)  + L(c) \\quad &\\text{assuming } a \\ne 0 \\\\\n=& a L\\left( x^2 + \\frac{b}{a} x \\right)  + L(c) &\\quad \\text{by property 2}  \\end{aligned}\n\\]\nthen we are basically stuck. Although we can force \\(x^2\\) to show up, we can’t at the same time force \\(3x\\) to show up; forcing a coefficient of \\(1\\) on \\(x^2\\) will always effect the coefficient of \\(x\\). Furthermore, we have no information about how to deal with \\(L(c)\\).\nSo, in this case, knowing that \\(L\\) is linear and knowing its value on one input is insufficient.",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases</span>"
    ]
  },
  {
    "objectID": "bases.html#what-is-the-minimal-amount-of-information-needed-to-unambiguously-describe-a-linear-function",
    "href": "bases.html#what-is-the-minimal-amount-of-information-needed-to-unambiguously-describe-a-linear-function",
    "title": "3  Bases",
    "section": "",
    "text": "Exercise 3.1 In the example above, is it enough to know that \\[\n\\begin{aligned}\nL(x^2) =& 1 , \\\\\nL(x) =& 1 ,\n\\end{aligned}\n\\]\nto be able to determine \\(L\\) on all of \\(P_2\\)?\nWhat if we know \\[\n\\begin{aligned}\nL(x^2) =& 1 , \\\\\nL(x) =& 1 ,\\\\\nL(1) =& -2\n\\end{aligned}?\n\\]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases</span>"
    ]
  },
  {
    "objectID": "bases.html#writing-vectors-relative-to-a-set-of-vectors",
    "href": "bases.html#writing-vectors-relative-to-a-set-of-vectors",
    "title": "3  Bases",
    "section": "Writing vectors relative to a set of vectors",
    "text": "Writing vectors relative to a set of vectors\nLet \\(X,Y \\in \\mathbb R^2\\) be given by \\(X:= (1,1)\\) and \\(Y:= (1,-1)\\). Suppose \\(L : \\mathbb R^2 \\to \\mathbb R\\) is linear and also that we know \\[\n\\begin{aligned}\nL(X) =&  1, \\\\\nL(Y) =&  -2\n\\end{aligned}.\n\\]\nCan we compute \\(L\\) on the vector \\((2,3)\\)? If the answer to this question were yes then there would have to be a way to express \\((2,3)\\) in terms of \\(X\\) and \\(Y\\) (since those are the only two values we know anything about). That is, there would be \\(a,b \\in \\mathbb R\\) such that\n\\[\n(2,3) = a X + b Y = a (1,1) + b(1 , -1) = (a , a) + ( b , -b ) = (a + b , a - b).\n\\]\nSo, \\(L\\) can be computed on \\((2,3)\\) if and only if the following system has a solution:\n\\[\n\\begin{aligned}\n2 =& a + b ,\\\\\n3 =& a - b\n\\end{aligned}.\n\\]\nWe can solve this system (solving for, say, \\(a\\) in the first and substituting the value in the second). We find that there is only one solution: \\(a = \\frac{5}{2} , b= -\\frac{1}{2}\\).\nUsing this information, how do we compute \\(L(2,3)\\)? Well, now we know that\n\\[\n(2,3) = \\frac{5}{2} (1 , 1) + \\left( -\\frac{1}{2} \\right) (1,-1)\n\\]\nso let’s just apply the function \\(L\\) to both sides of this equation:\n\\[\n\\begin{aligned}\nL(2,3) =& L \\left[ \\frac{5}{2} (1 , 1) + \\left( -\\frac{1}{2} \\right) (1,-1) \\right] \\\\\n=& L \\left[ \\frac{5}{2} (1 , 1) \\right] + L \\left[ \\left( -\\frac{1}{2} \\right) (1,-1) \\right] \\\\\n=& \\frac{5}{2} L   (1 , 1) + \\left( -\\frac{1}{2} \\right) L  (1,-1) \\\\\n=& \\frac{5}{2} 1 + \\left( -\\frac{1}{2} \\right) (-2) \\\\\n=& \\frac{5}{2}  + 1\\\\\n=& \\frac{7}{2}\n\\end{aligned}\n\\]\n\nExercise 3.2 For each \\(=\\)-symbol in the computation above, write the assumption, property, or rule that tells us we are allowed to conclude the left hand side is equal to the right hand side.\n\nThe above computation shows that the answer to our question is yes, this amount of information about \\(L\\) is sufficient to compute \\(L(2,3)\\). Notice that this is the case even though we were not given the formula for \\(L\\).\nYou may want to convince yourself that the vector \\((2,3)\\) above was not special; given the information we have about \\(L\\), we can compute \\(L(k,l)\\) for any \\((k,l) \\in \\mathbb R^2\\).\n\nExercise 3.3 If we know that \\(L : \\mathbb R^2 \\to \\mathbb R\\) is linear and that\n\\[\nL(0,1) = 1 , \\quad L(0,2) = 2,\n\\]\ncan we compute \\(L(1,2)\\)? If yes, compute it. If no, why not?\n\n\nExercise 3.4 If we know that \\(L : \\mathbb R^2 \\to \\mathbb R\\) is linear and that\n\\[\nL(0,1) = 2 , \\quad L(1,1) = 2,\n\\]\ncan we compute \\(L(1,2)\\)? If yes, compute it. If no, why not?\n\n\nLinear combinations\nThe trick to computing \\(L(2,3)\\) in the previous section was to rewrite \\((2,3)\\) in the form\n\\[\n(2,3) = a (1,1) + b(1 , -1).\n\\]\nThis is a good ansatz1 because we are able to split over the \\(+\\) and pull the \\(a\\) and \\(b\\) out (using the fact that \\(L\\) is linear).\n\nDefinition 3.1 If \\(X_1, X_2 , \\ldots , X_n\\) is a collection of vectors in the vector space \\(V\\), then any expression of the form\n\\[\na_1 X_1 + a_2 X_2 + \\cdots + a_n X^n, \\quad a_i \\in F\n\\]\nis called a linear combination of \\(X_1, X_2 , \\ldots , X_n\\).\n\nWe then have the following generalization of the computation we did in the example above:\n\nTheorem 3.1 Let \\(L: A \\to B\\) be linear. These two statements are equivalent:\n\nWe know the value of \\(L\\) on a set of vectors \\(X_1 ,  X_2 , \\ldots , X_n \\in A\\). That is,\n\\[\n\\begin{aligned}\nL(X_1) =& Y_1 \\\\\nL(X_2) =& Y_2 \\\\\n\\vdots & \\\\\nL(X_n) =& Y_n\n\\end{aligned}\n\\]\nfor some \\(Y_i \\in B\\).\nWe know the value of \\(L\\) on any linear combination of \\(X_1 ,  X_2 , \\ldots , X_n\\).\n\n\nThe proof \\(\\implies\\) is a straightforward application of properties 1 and 2 of linear functions. You might try to generate the justification yourself. The proof of \\(\\impliedby\\) is an application of proof by contradiction.\n\n\n\n\n\n\nNote\n\n\n\nTheorem 3.1 is not a tautology!\nThe set \\(\\{X_1 ,  X_2 , \\ldots , X_n\\}\\) is a finite set (think of \\(\\{(1,1) , (1,-1)\\}\\) which contains only 2 elements).\nThe set of linear combinations of \\(\\{X_1 ,  X_2 , \\ldots , X_n\\}\\) is infinite (if the field of scalars is infinite, which in the case of \\(\\mathbb R\\) or \\(\\mathbb C\\) it is).\nThe set of linear combinations of \\(\\{(1,1) , (1,-1)\\}\\) contains, for example, \\[\n\\begin{aligned}\n&(1,1),(2,2),(a,a) \\forall a \\in \\mathbb R ,\\\\\n&(\\pi,e),\\left(\\sqrt{2},2025^{1/3}\\right),\\\\\n&a(1,1) + b(1,-1) = (a+b,a-b) \\forall a,b \\in \\mathbb R.\n\\end{aligned}\n\\] In particular, this set is infinite.\n\n\n\n\nThe span of a set of vectors\nWe will use the words “is a linear combination of” or “can be expressed as a linear combination of” quite often, so we introduce some terminology to shorthand this.\n\nDefinition 3.2 If \\(V\\) is a vector space over a field \\(F\\), \\(X_1 , \\ldots , X_n \\in V\\), and \\(W\\) is expressible as a linear combination of \\(\\{X_1 , \\ldots , X_n \\}\\) we say that \\(W\\) is in the span of \\(\\{X_1 , \\ldots , X_n \\}\\). We can write this\n\\[\nW \\in \\Span \\{X_1 , \\ldots , X_n \\}.\n\\]\nWe also refer to the following set as the span of \\(\\{X_1 , \\ldots , X_n \\}\\): \\[\n\\Span \\{X_1 , \\ldots , X_n \\} := \\{ a_1 X_1 + \\cdots + a_n X_n \\mid a_i \\in F \\}.\n\\]\n\nSo, for example, \\((0,2025) \\in \\Span \\{(0,1),(0,6)\\}\\), but the vector \\((17,9) \\not \\in \\Span \\{(0,1),(0,6)\\}\\).\n\nExercise 3.5 Let \\((a,b) \\in \\mathbb R^2\\). Is \\((a,b) \\in \\Span \\{ (1,1) , (1,-1) \\}\\)? Why or why not?\n\n\nExercise 3.6 Let \\((a,b) \\in \\mathbb R^2\\). Is \\((a,b) \\in \\Span \\{ (1,2) , (1 ,5) , (1 , -3) \\}\\)? Why or why not?\n\nGiven the above definition, we can rephrase Theorem 3.1:\n\nTheorem 3.2 Let \\(L: A \\to B\\) be linear. These two statements are equivalent:\n\nWe know the value of \\(L\\) on a set of vectors \\(X_1 ,  X_2 , \\ldots , X_n \\in A\\).\nWe know the value of \\(L\\) on \\(\\Span \\{ X_1 ,  X_2 , \\cdots , X_n \\}\\).\n\n\nThere isn’t really anything to prove here; this is exactly Theorem 3.1 except with “any linear combination of” replaced by \\(\\Span\\), which is Definition 3.2.\n\nExercise 3.7 Is \\(\\Span \\{ 1 + x , x + x^2 \\} = P_2\\)? Why or why not?\n\n\nExercise 3.8 Show that \\(\\Span \\{ 1 + x , x + x^2, x^2 , x^3 \\} = P_3\\) by finding a representation of \\[\nax^3 + b x^2 + c x + d\n\\] (which is the form of a generic element of \\(P_3\\)) as a linear combination of \\(\\{ 1 + x , x + x^2, x^2 , x^3 \\}\\).\n\n\nExercise 3.9 Suppose we know that \\(F'(x) = f(x)\\) and \\(G'(x) = g(x)\\) (that is, \\(F,G\\) have derivatives, and we give the names \\(f,g\\) to those derivatives). Recall the definition of the operator \\(D\\) for differentiation: \\[\nD(h) := h'(x),\n\\] and that we already know \\(D : \\{ \\text{differentiable functions} \\} \\to \\{ \\text{ functions}\\}\\) is linear.\nIs it true that \\(D(F(x) + 14G(x))=f(x) + 16 g(x)\\)? Did you use any properties about \\(D\\) other than linearity to conclude that?\nSuppose \\(G(x) &gt; 0 , \\forall x\\). Is it true that \\[\nD\\left [ \\frac{F(x)}{G(x)} \\right] = \\frac{f(x) G(x) - F(x)g(x)}{\\left[G(x)\\right]^2}?\n\\] Did you use any properties about \\(D\\) other than linearity to conclude that?\n\n\n\nLinear independence\nSuppose we have a collection of vectors \\(\\{ X_1 , \\ldots , X_n \\} \\in \\mathbb R^2\\). We have seen examples where\n\n\\(\\{ X_1 , \\ldots , X_n \\}\\) consists of two vectors and \\(\\Span \\{ X_1 , \\ldots , X_n \\}\\) is not all of \\(\\mathbb R^2\\)\n(this happened when \\(\\{ X_1 , \\ldots , X_n \\} = \\{(0,1) , (0,2)\\}\\)),\n\\(\\{ X_1 , \\ldots , X_n \\}\\) consists of two vectors and \\(\\Span \\{ X_1 , \\ldots , X_n \\}\\) is equal to \\(\\mathbb R^2\\)\n(this happened when \\(\\{ X_1 , \\ldots , X_n \\} = \\{(1,1) , (1,-1)\\}\\)),\n\\(\\{ X_1 , \\ldots , X_n \\}\\) consists of three vectors and \\(\\Span \\{ X_1 , \\ldots , X_n \\}\\) is equal to \\(\\mathbb R^2\\)\n(this happened when \\(\\{ X_1 , \\ldots , X_n \\} = \\{(1,2) , (1 , 6) , (1,-3)\\}\\)).\n\nIn the last case, we got something extra; there were infinitely many ways of representing an arbitrary element \\((a,b) \\in \\mathbb R^2\\) as a linear combination of \\(\\{(1,2) , (1 , 6) , (1,-3)\\}\\). In the second case, we get a unique way of representing each vector as a linear combination, and in the first case we don’t even get any way to represent some vectors as a linear combination.\nFrom Definition 3.2 we now can say that the distinction between the first case and the other two is that\n\\[\n\\Span \\{(1,1) , (1,-1)\\} = \\Span \\{(1,2) , (1 , 6) , (1,-3)\\} = \\mathbb R^2\n\\]\nbut that\n\\[\n\\Span \\{(0,1) , (0,2)\\} \\ne \\mathbb R^2.\n\\]\nFrom Theorem 3.2, we know that if we know a linear function on the two vectors\n\\[\n(1,1) , (1,-1)\n\\]\nor on the three vectors\n\\[\n(1,2) , (1 , 6) , (1,-3)\n\\]\nthen we know its value on all inputs \\((a,b) \\in \\mathbb R^2\\).\nWhat is the criterion that distinguishes the second case from the last one? That is, is there a property of a set of vectors \\[\n\\{(1,2) , (1 , 6) , (1,-3)\\}\n\\] from the set of vectors \\[\n\\{(1,1) , (1,-1)\\}\n\\] that tells is that we get a unique linear combination in the latter case, but a nonunique linear combination in the former case? It turns out to be the following definition, and we will see why in a later part of the book.\n\nDefinition 3.3 Let \\(V\\) be a vector space and \\(\\{ X_1 , \\ldots , X_n \\} \\subset V\\). We say that the set \\(\\{ X_1 , \\ldots , X_n \\}\\) is linearly independent if, for each \\(i \\in \\{ 1 , \\ldots , n \\}\\),\n\\[\nX_i \\not \\in \\Span \\{X_1 , X_2 , \\cdots , X_{i-1} , X_{i + 1} , \\cdots , X_n\\}.\n\\]\nThis is another way of saying that \\(X_i\\) is not in the span of all the \\(\\{ X_j \\}\\) with \\(X_i\\) removed.\n\n\n\nA basis for a vector space\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases</span>"
    ]
  },
  {
    "objectID": "bases.html#applying-linear-functions-using-basis-representation",
    "href": "bases.html#applying-linear-functions-using-basis-representation",
    "title": "3  Bases",
    "section": "Applying linear functions using basis-representation",
    "text": "Applying linear functions using basis-representation\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases</span>"
    ]
  },
  {
    "objectID": "bases.html#other-bases",
    "href": "bases.html#other-bases",
    "title": "3  Bases",
    "section": "Other bases",
    "text": "Other bases\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases</span>"
    ]
  },
  {
    "objectID": "bases.html#dimension",
    "href": "bases.html#dimension",
    "title": "3  Bases",
    "section": "Dimension",
    "text": "Dimension\n[TODO]",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases</span>"
    ]
  },
  {
    "objectID": "bases.html#footnotes",
    "href": "bases.html#footnotes",
    "title": "3  Bases",
    "section": "",
    "text": "This is a German word that, in math, means a guess with some free parameters (in this case, \\(a\\) and \\(b\\)) in it. We compute with this guess, and then later find values of the parameters that make the computation valid and also solve our problem. If we cannot find such values, the we have essentially proved that there is no solution to our problem with the form given in the ansatz (although there may be a solution of another form if our ansatz is not general enough).↩︎",
    "crumbs": [
      "Vector Spaces",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Axler, Sheldon. 2024. Linear Algebra Done Right. Undergraduate\nTexts in Mathematics. Springer, Cham. https://doi.org/10.1007/978-3-031-41026-0.\n\n\nDummit, David S., and Richard M. Foote. 2004. Abstract Algebra.\nThird Ed. John Wiley & Sons, Inc., Hoboken, NJ.\n\n\nHefferon, Jim. 2022. Linear Algebra. Fourth Ed. https://hefferon.net/linearalgebra/.\n\n\nTreil, Sergei. 2024. Linear Algebra Done Wrong. https://sites.google.com/a/brown.edu/sergei-treil-homepage/linear-algebra-done-wrong.",
    "crumbs": [
      "References"
    ]
  }
]