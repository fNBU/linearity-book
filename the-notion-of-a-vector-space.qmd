# The notion of a Vector Space

If $L : A \to B$ is linear, what must be true about $A$ and $B$?

Let's go back to the definition of a linear function.
A function is linear if and only if it satisfies the following two properties:

$$
\begin{aligned}
\text{Property 1:} \quad & L(a x )= a L(x) \\
\text{Property 2:} \quad & L( x + y ) = L(x) + L(y).
\end{aligned}
$$

Let's list a few things that must be true to arrive at these expressions:

- there are terms (like $a$ in property 1) that we can factor through $L$
- there are terms (like $x,y$ in properties 1 and 2) that we cannot factor through $L$
- there is some kind of addition on the $x$s and $y$s

There are perhaps some more properties that would be nice, and that are true about all the domains and codomains of linear functions we have seen so far:

- the $a$s (that we can pull through $L$) have nice algebraic properties ($+,-,\times, \text{division}$)
- the $+$ operation on the $x$s and $y$s has some nice properties, too (existence of an identity, commutativity, etc)

Eventually, mathematicians (who were working with linear functions intuitively) worked out the minimal set of facts that one needs about the domain and codomain of a linear function for everything to be coherent.
Here it is

## Definition

::: {#def-vector-space}

A *vector space* is a set $V$ of *vectors* and a set $F$ of *scalars* that satisfy the following properties

- the set of scalars $F$ is a field (see [the next section](#scalars) for more information on what we mean by field),
- there is a function $+ \colon V \times V \to V$, called *vector addition* that 
  - is associative: $(X+Y) + Z = X + (Y + Z)$,
  - is commutative: $X+Y = Y+X$,
  - has a $0$ (an additive identity) which means that $0$ satisfies $0 + X = X$ for all $X \in V$,
  - has negatives (additive inverses) which means that for each $X \in V$ there is an element $Y \in V$ such that $X + Y = 0$ (one can prove that there is only one inverse of $X$, and this is usually written $-X$),
- there is a function $\cdot \colon F \times V \to V$, called *scalar multiplication* such that
  - scalar multiplication associates with field multiplication (that is, $\forall a , b \in F, X \in V$ it's true that $(ab)X = a (bX)$)
  - the multiplicative unit in $F$ is a unit for scalar multiplication (that is, $\forall X \in V$ we have $1 X = X$)
- the following distributive laws hold:
  - $(a+b) X = aX + bX , \quad \forall a, b\in F , X \in V$
  - $a(X+Y) = aX + aY , \quad \forall a \in F ,  X,Y \in V$

:::

::: {.callout-tip title="Notation"}

Above, we used the symbol $\forall$.
This is a symbol that mathematicians use that literally just means "for all".

So, as an example $x \in P_3 , \forall x \in P_2$ is read as "for all x in the set of second degree polynomials, x is in the set of third degree polynomials", which is just a wordy way of saying that all second degree polynomials are third degree polynomials.

:::

Note that the notation $+ \colon V \times V \to V$ implies that $V$ is, in particular, closed under addition (since $V$ is the codomain of $+$; addition cannot by definition return an element not in $V$). 
Similarly we can conclude that $V$ must be closed under scalar multiplication.

All of the properties above hold (with the field of scalars $F$ set to $\mathbb R$) for all the spaces we have used so far as the domain and codomain of a linear function. (You may want to check this for yourself quickly. Most of these facts obviously hold for polynomials, functions, points in $\mathbb R^2$, and $\mathbb R$ itself. However, one must still check all of them before using a set as the domain or codomain of a linear function.)

Not all sets of mathematical objects satisfy these properties, though.
An example is the set of points on the sphere.
It turns out that there is no way (although the proof will have to wait for a future chapter) to turn that set into a vector space.

The way you should think about this definition is the following:

- we are **allowed to use** a set $A$ satisfying all of the properties in @def-vector-space as the domain or codomain of a linear function, but
- a set $A$ that fails to satisfy any of the properties above **can never be used** as the domain or codomain of a linear function. If you tried to use it in this way, you would eventually run into statements that make no sense.[^ring-note]

[^ring-note]: It is the case that some maps between spaces that are weaker than vector spaces have defining properties 1 and 2. The spaces are a generalization of vector spaces called [modules](https://en.wikipedia.org/wiki/Module_(mathematics)) and the maps are called [module homomorphisms](https://en.wikipedia.org/wiki/Module_homomorphism). If you look at the definition of module homomorphism on wikipedia, you'll see the same two equations we used to define linear functions.

An example of the kind of problem you can run into is the following: Let $L:A \to B$ be linear and suppose $A$ is a vector space as defined above but that $B$ fails to have an additive identity (that is, there is no zero vector in $B$).
We can compute

$$
L(0) = 0L(0) = 0 (?) = 0
$$
where the $?$ stands for whatever $L(0)$ maps to.
Notice that in the end, it doesn't matter because we conclude that $L(0) = 0 \in B$.
But this is nonsensical since we assumed there was no zero vector in $B$.

::: {#exr-r-is-a-vspace}

Check that $\mathbb R$ forms a vector space (the set of scalars is $\mathbb R$).

:::

::: {#exr-r2-is-a-vspace}

Check that $\mathbb R^2$ is a vector space.

:::

## Scalars

In the above definition, we said that the scalars for a vector space must come from a field.
What does this mean?

For the purposes of this text, our field will always be $\mathbb R$ or the field of complex numbers:

$$
\mathbb C := \{ a + b i \mid a,b \in \mathbb R \}
$$

where the symbol $i$ has the property that $i^2 = -1$ (and so we sometimes write it as $\sqrt{-1}$).
These two sets are the set of scalars for most vector spaces found in applications in the wild.

::: {#exr-c-is-a-vspace-over-r}

Check that $\mathbb C$ can be thought of as a vector space with field of scalars $\mathbb R$.

:::

::: {#def-polynomial-space}

Let $P_n$ be the set (which we mentioned informally in the previous chapter)

$$
\left\{ a_{n} x^n + a_{n-1} x^n + \cdots + a_1 x + a_0 \mid a_n , a_{n-1}, \ldots , a_1 , a_0 \in \mathbb R \right\}.
$$

This is called the *set of polynomials of degree $n$*.

:::

::: {#exr-pn-is-a-vspace}

Is $P_n$ a vector space for each $n \in \mathbb N$?

:::

From the previous two examples, you might notice that the definitions

$$
\mathbb C := \{ a + b i \mid a,b \in \mathbb R \}
$$

and 

$$
P_n := \left\{ a_{n} x^n + a_{n-1} x^n + \cdots + a_1 x + a_0 \mid a_n , a_{n-1}, \ldots , a_1 , a_0 \in \mathbb R \right\}
$$

look similar, and they give you a hint about how to think of these as vector spaces; in both cases, they are constructed as the sum of things with coefficients in $\mathbb R$.
We will investigate this further in the next chapter.

### Fields

In case you are interested, the full definition of a field can be found on [Wikipedia](https://en.wikipedia.org/wiki/Field_(mathematics)#Classic_definition).
A field that you already know about, but that is not $\mathbb R$ or $\mathbb C$ is the field of rational numbers:

$$
\mathbb Q := \left\{ \left. \frac{a}{b}  \right| a,b \in \mathbb Z ,  b \ne 0  \right\}.
$$

There are esoteric examples of fields, and also algebraic structures stranger than fields.
If this is interesting to you, you might try to take a course in Abstract Algebra.
I learned this topic from Liz Stanhope and @dummit-foote.

In computing, real numbers (which do form a field) are often represented by 32 or 64 bit floating point numbers.
It is perhaps interesting to know that floating point numbers (in any number of bits) (which are encountered **often** in the wild) do not form a field because addition of floating point numbers is not associative.

## Subspaces

We have seen in this chapter that if you want to do something with a linear function, you better check first that its domain and codomain are vector spaces.
It's quite a lengthy process to verify all the properties in @def-vector-space, even if in many cases it is not difficult to verify each one individually.
We are in need of machines that allow us to know that something is a vector space more easily.
That is what this and the next section are about.

How do we know that a set $W \subset V$ is a vector space?

::: {.callout-tip title="Notation"}

Above, we used the symbol $\subset$.
This just means "is a sub set of", for example $\mathbb Z \subset \mathbb Q$ means "the set of integers is a subset of the set of rational numbers."

When we say "A is a subset of B" or, equivalently write $A \subset B$, we mean that everything in $A$ is also in $B$.

We will sometimes write $A \supset B$ to mean that $B$ is a subset of $A$ or that "A has B as a subset."

:::

We've already seen some examples of this.
For example, we know that differentiable functions form a vector space, and also the set $P_n$ forms a vector space.
Since every polynomial is differentiable, it follows that $P_n \subset \{ \text{differentiable functions}\}$.
So $P_n$ is an example of a subset of a vector space that turns out to be a vector space itself.

Let's introduce some terminology for this situation.

::: {#def-vector-subspace}

A subset $W$ of a vector space $V$ is called a (vector) *subspace* if $W$ is itself a vector space (with field of scalars $F$, addition, multiplication, zero vector inherited from $V$).

:::

We have already seen an example of a subspace above.
A nonexample is the set of points in the plane at distance 1 from the origin.
This set doesn't satisfy the closure assumptions of @def-vector-space.

It is a bit remarkable (although not difficult to prove) that we actually only need to check the closure assumptions.
The following theorem encodes this fact.

::: {#thm-vector-subspace}

A subset $W$ of a vector space $V$ is itself a vector space (with field of scalars $F$, addition, multiplication, zero vector inherited from $V$) iff the following two properties hold:

1. $W$ is closed under scalar multiplication: $aX \in W \quad \forall a \in F, X \in W$
1. $W$ is closed under vector addition: $X+Y \in W \quad \forall X,Y \in W$

:::

We won't do the proof here (you can go to e.g. @hefferon to see it), but we will sketch it.

Since the scalars, and the operations $\cdot , +$ are inherited from the ambient space $V$, they are known to satisfy associativity, commutativity, existence of inverses and zeros, etc (because we had to prove those things when we showed that $V$ was a vector space).
However, in @def-vector-space, we wrote $+ \colon V \times V \to V$.
That is, $+$ takes two vectors in $V$ and returns a vector in $V$.
Since $W \subset V$, we know that we can use elements of $W$ as inputs to $+$.
That is, we know $+ \colon W \times W \to V$, but we cannot know addition of vectors always lands in the subset $W$.
The first assumption above tells us this.
Similarly for scalar multiplication.

For the proof in the opposite direction, notice that for $W$ to be a vector space we must have $+ \colon W \times W \to W$.
If addition is not closed, then $+$ has the wrong codomain, and $W$ isn't a vector space.
Similarly if $W$ is not closed under scalar multiplication.

### Easier subspaces

Notice that we can use @thm-vector-subspace to quite easily prove (assuming we know differentiable functions form a vector space) that $P_n$ is a vector space.
We can also conclude easily that, for example,

$$
\{ a \sin x + b \cos x \mid a,b \in \mathbb R \}
$$

is a vector space.

::: {#exr-arrows-subspace}

Is the set of arrows that are either fully horizontal or fully vertical a vector subspace of the vector space of arrows in 2D?
Why? 

:::

::: {#exr-monomials}

Consider the set of monomials

$$
\{ 1 , x , x^2 , x^3 , \ldots  \} .
$$

If I pick any two of these (say $x^k$ and $x^l$ with $k \ne l$, eg $x^{17}$ and $x^{2025}$) and form the set 

$$
\{ a x^k + b x^l \mid a,b \in \mathbb R \} ,
$$

is this a vector subspace of the space of differentiable functions?
Why?

:::

## The space of lists of length $n$

We have seen that the following are vector spaces: $\mathbb R$ over $\mathbb R$, $\mathbb R^2$ over $\mathbb R$, $\mathbb C$ over $\mathbb C$.

There is a very powerful theorem that we can prove to generate many vector spaces that are of a similar form.


::: {#thm-list-spaces}

Let $F$ be any field.
Define the following set

$$
F^n := \{ (a_1 , a_2 , \cdots , a_n) \mid a_i \in F \}.
$$

This is the space of lists of length $n$, with members from $F$.

Let $+ : F^n \times F^n \to F^n$ be defined by

$$
(a_1 , a_2 , \cdots , a_n) + (b_1 , b_2 , \cdots , b_n) = (a_1 + b_1 , a_2 + b_2 , \cdots , a_n + b_n )
$$

and let $\cdot : F \times F^n \to F^n$ be defined by 

$$
x (a_1 , a_2 , \cdots , a_n) = (xa_1 , xa_2 , \cdots , xa_n).
$$

Then $F^n$ with these two operations (and field of scalars $F$) is a vector space.

:::

The proof is a straightforward check of all the items in @def-vector-space.

Note that $\mathbb R, \mathbb R^2 , \mathbb C$ can be proved to be vector spaces using this theorem.

To develop your intuition for what this looks like, let's do a multiplication in $\mathbb C^4$:

$$
\begin{aligned}
& i (1 , i , 1 + i , 2 - 3 i)\\
=& (i , i^2 , i + i^2 , 2 i - 3 i^2) \\
=& ( i , -1 , i -1 , 2i +3).
\end{aligned}
$$

Note that this theorem only works if $F^n$ is a vector space over $F$.
So, above, $\mathbb C^4$ is a vector space with scalars $\mathbb C$.

::: {#exr-cn-rn}

Is $\mathbb R^n$ a vector subspace of $\mathbb C^n$?
Think very carefully about @def-vector-subspace.

:::