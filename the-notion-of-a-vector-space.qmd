# The notion of a Vector Space

If $L : A \to B$ is linear, what must be true about $A$ and $B$?

Let's go back to the definition of of a linear function.
A function is linear if and only if it satisfies the following two properties:

$$
\begin{aligned}
\text{Property 1:} \quad & L(a x )= a L(x) \\
\text{Property 2:} \quad & L( x + y ) = L(x) + L(y).
\end{aligned}
$$

Let's list a few things that must be true to arrive at these expressions:

- there are terms (like $a$ in property 1) that we can factor through $L$
- there are terms (like $x,y$ in properties 1 and 2) that we cannot factor through $L$
- there is some kind of addition on the $x$s and $y$s

There are perhaps some more properties that would be nice, and that are true about all the domains and codomains of linear functions we have seen so far:

- the $a$s (that we can pull through $L$) have nice algebraic properties ($+,-,\times, \text{division}$)
- the $+$ operation on the $x$s and $y$s has some nice properties, too (existence of an identity, commutativity, etc)

Eventually, mathematicians (who were working with linear functions intuitively) worked out the minimal set of facts that one needs about the domain and codomain of a linear function for everything to be coherent.
Here it is

## Definition

::: {#def-vector-space}

A *vector space* is a set $V$ of *vectors* and a set $F$ of *scalars* that satisfy the following properties

- the set of scalars $F$ is a field (see [the next section](#scalars) for more information on what we mean by field),
- there is a function $+ \colon V \times V \to V$, called *vector addition* that 
  - is associative: $(X+Y) + Z = X + (Y + Z)$,
  - is commutative: $X+Y = Y+X$,
  - has a $0$ (an additive identity) which means that $0$ satisfies $0 + X = X$ for all $X \in V$,
  - has negatives (additive inverses) which means that for each $X \in V$ there is an element $Y \in V$ such that $X + Y = 0$ (one can prove that there is only one inverse of $X$, and this is usually written $-X$),
- $V$ is closed under vector addition and scalar multiplication, which means that 
  - for every $X,Y \in V$, $X + Y$ is also in $V$
  - for every $a \in F$ and $X \in V$, $aX$ is also in $V$
- the following distributive laws hold:
  - $(a+b) X = aX + bX , \quad \forall a, b\in F , X \in V$
  - $a(X+Y) = aX + aY , \quad \forall a \in F ,  X,Y \in V$

:::

::: {.callout-tip title="Notation"}

Above, we used the symbol $\forall$.
This is a symbol that mathematicians use that literally just means "for all".

So, as an example $x \in P_3 , \forall x \in P_2$ is read as "for all x in the set of second degree polynomials, x is in the set of third degree polynomials", which is just a wordy way of saying that all second degree polynomials are third degree polynomials.

:::

All of the properties above hold (with the field of scalars $F$ set to $\mathbb R$) for all the spaces we have used so far as the domain and codomain of a linear function. (You may want to check this for yourself quickly. Most of these facts obviously hold for polynomials, functions, points in $\mathbb R^2$, and $\mathbb R$ itself. However, one must still check all of them before using a set as the domain or codomain of a linear function.)

Not all sets of mathematical objects satisfy these properties, though.
An example is the set of points on the sphere.
It turns out that there is no way (although the proof is hard) to turn that set into a vector space.

The way you should think about this definition is the following:

- a set $A$ satisfying all of the properties in @def-vector-space **may** be used as the domain or codomain of a linear function, but
- a set $A$ that fails to satisfy any of the properties above **can never be used** as the domain or codomain of a linear function. If you tried to use it in this way, you would eventually run into statements that make no sense.[^ring-note]

[^ring-note]: It is the case that some maps between spaces that are weaker than vector spaces have defining properties 1 and 2. The spaces are a generalization of vector spaces called [modules](https://en.wikipedia.org/wiki/Module_(mathematics)) and the maps are called [module homomorphisms](https://en.wikipedia.org/wiki/Module_homomorphism). If you look at the definition of module homomorphism on wikipedia, you'll see the same two equations we used to define linear functions.

An example of the kind of problem you can run into is the following: Let $L:A \to B$ be linear and suppose $A$ is a vector space as defined above but that $B$ fails to have an additive identity (there is no zero vector in $B$).
We can compute

$$
L(0) = 0L(0) = 0 (?) = 0
$$
where the $?$ stands for whatever $L(0)$ maps to.
Notice that in the end, it doesn't matter because we conclude that $L(0) = 0 \in B$.
But this is nonsensical since we assumed there was no zero vector in $B$.

::: {#exr-r-is-a-vspace}

Check that $\mathbb R$ forms a vector space (the set of scalars is $\mathbb R$).

:::

::: {#exr-r2-is-a-vspace}

Check that $\mathbb R^2$ is a vector space.

:::

## Scalars

In the above definition, we said that the scalars for a vector space must come from a field.
What does this mean?

For the purposes of this text, our field will always be $\mathbb R$ or the field of complex numbers:

$$
\mathbb C := \{ a + b i \mid a,b \in \mathbb R \}
$$

where the symbol $i$ has the property that $i^2 = -1$ (and so we sometimes write it as $\sqrt{-1}$).
These two sets are the set of scalars for most vector spaces found in applications in the wild.

::: {#exr-c-is-a-vspace-over-r}

Check that $\mathbb C$ can be thought of as a vector space with field of scalars $\mathbb R$.

:::

::: {#def-polynomial-space}

Let $P_n$ be the 

$$
\left\{ a_{n} x^n + a_{n-1} x^n + \cdots + a_1 x + a_0 \mid a_n , a_{n-1}, \ldots , a_1 , a_0 \in \mathbb R \right\}.
$$

This is called the *set of polynomials of degree $n$*.

:::

::: {#exr-pn-is-a-vspace}

Is $P_n$ a vector space for each $n \in \mathbb N$?

:::

From the previous two examples, you might notice that the definitions

$$
\mathbb C := \{ a + b i \mid a,b \in \mathbb R \}
$$

and 

$$
P_n := \left\{ a_{n} x^n + a_{n-1} x^n + \cdots + a_1 x + a_0 \mid a_n , a_{n-1}, \ldots , a_1 , a_0 \in \mathbb R \right\}
$$

look similar, and they give you a hint about how to think of these as vector spaces; in both cases, they are constructed as the sum of things with coefficients in $\mathbb R$.
We will investigate this further in the next chapter.

### Abstract algebra

In case you are interested, the full definition of a field can be found on [Wikipedia](https://en.wikipedia.org/wiki/Field_(mathematics)#Classic_definition).
A field that you already know about, but that is not $\mathbb R$ or $\mathbb C$ is the field of rational numbers:

$$
\mathbb Q := \left\{ \left. \frac{a}{b}  \right| a,b \in \mathbb Z ,  b \ne 0  \right\}.
$$

There are esoteric examples of fields, and also algebraic structures stranger than fields.
If this is interesting to you, you might try to take a course in Abstract Algebra.
I learned this topic from @dummit-foote.

In computing, real numbers (which do form a field) are often represented by 32 or 64 bit floating point numbers.
It is perhaps interesting to know that floating point numbers (in any number of bits) (which are encountered **often** in the wild) do not form a field because addition of floating point numbers is not associative.